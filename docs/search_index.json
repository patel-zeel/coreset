[["index.html", "IN792: Coresets Coresets: 1 Preface", " IN792: Coresets Zeel B Patel 2021-03-31 Coresets: 1 Preface This is a report generated for work done in IN-792 project with Prof. Anirban Dasgupta in Semester-II of academic year 2020-2021. "],["coresets.html", "Coresets: 2 Coresets 2.1 What are coresets?", " Coresets: 2 Coresets 2.1 What are coresets? A well-written definition of coresets is the following by Bachem, Lucic, and Krause (2017), Coresets are small, weighted summaries of large data sets such that solutions found on the summary itself are provably competitive with solutions found on the full data set. More intuitively, a coreset is a small weighted sample of the larger data that can represent the original data with a good approximation on a particular model/algorithm. Coresets are extremely useful for models that do not scale well with data. If a model can not be optimized further on time-complexity, using coresets is natural option to cope up with training time. Theoretical guerantee is a unique property about coresets over other approximation techniques, which makes it robust for practical usage. In next chapter we shall see practical introduction to coresets for KMeans. "],["coresets-for-kmeans-theory.html", "Coresets: 3 Coresets for KMeans (Theory) 3.1 Problem formulation 3.2 Uniform sampling 3.3 Importance sampling", " Coresets: 3 Coresets for KMeans (Theory) 3.1 Problem formulation In a KMeans clustering problem, we want to cluster the dataset \\(X \\in R^d\\) of cardinality \\(N\\) in \\(K\\) seperate clusters. If \\(Q\\) is a set of cluster centers for KMeans in this problem, we can define the cost as following, \\[ cost(X, Q) = \\sum\\limits_{x\\in X}\\min\\limits_{q\\in Q}||x-q||^2_2 \\] If \\(C\\) of cardinality \\(N&#39;\\) is a weighted coreset constructed from dataset \\(X\\), \\(N&#39;&lt;&lt;N\\) is our desired property. \\(C\\) is a valid \\(\\varepsilon\\)-coreset of \\(X\\) if the following property holds for any \\(Q\\) with high probability (can be quantified), \\[ |cost(X, Q) - cost(C,Q)| \\le \\varepsilon cost(X,Q) \\] There is an another consequence of the above theorem which is more useful in practice. If \\(Q^*_X\\) is optimal cluster centers obtained by executing KMeans on full dataset \\(X\\) and \\(Q^*_C\\) is optimal cluster centers obtained by executing KMeans on coreset \\(C\\), the following property holds with high probability, \\[ cost(X,Q^*_X) \\le cost(X, Q^*_C) \\le \\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X) \\] Note that, in practice, computing \\(Q^*_X\\) is not feasible thus we are willing bear a higher cost upto \\(\\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X)\\) at benefit of reduced computational time. This property ensures that cost on the coreset stays within the defined upper bound without actually need of computing \\(Q^*_X\\) (which is obvious but just being explicit). Now, the question is how to construct the coreset \\(C\\) such that above properties hold with high probability (yes, that is important. Nothing is deterministic in the probabilistic world). Let us test a naive way first. 3.2 Uniform sampling 3.3 Importance sampling In the next chapter, we will actually implement the coresets for KMeans clustering step by step. "],["coresets-for-kmeans-practical.html", "Coresets: 4 Coresets for KMeans (Practical) 4.1 Pseudo-data with 4 clusters", " Coresets: 4 Coresets for KMeans (Practical) In this practical, we will try to create an \\(\\varepsilon\\)-coreset (\\(\\varepsilon=0.3\\)) for KMeans clustering. Let us import some packages and set up a few useful functions, from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from scipy.spatial.distance import cdist, pdist import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) rc(&#39;font&#39;, size=16) rc(&#39;text&#39;, usetex=True) def plot_essentials(): # essential code for every plot hand, labs = plt.gca().get_legend_handles_labels() if len(hand)&gt;0: plt.legend(hand, labs); plt.tight_layout(); plt.show() plt.close() 4.1 Pseudo-data with 4 clusters Now, we generate a pseudo-dataset with 4 clusters. N = 100 K = 4 d = 2 epsilon = 0.3 c4X, c4y = make_blobs(n_samples=N, centers=K, n_features=d, random_state=0, cluster_std=0.8) plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We run the KMeans algorithm on full dataset to calculate \\(cost(X, Q^*_X)\\). Note that, in practice, this is infeasible because of superlinear time-complexity, but, we perform this step to have a comparison between \\(cost(X, Q^*_X)\\) and \\(cost(X, Q^*_C)\\). full_model = KMeans(n_clusters=K, random_state=0) full_model.fit(c4X); cost_QX = full_model.inertia_/N Optimal cost on the full dataset is \\(cost(X, Q^*_X)=\\) 1.16. Q_X = full_model.cluster_centers_ plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, label=&#39;$Q^*_X$&#39;, c=&#39;green&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plt.title(&#39;Optimal cluster centers on the original dataset&#39;); plot_essentials() Next step is running the \\(D^2\\) sampling algorithm on the full dataset to get the approximate centers \\(Q^*_B\\). To ensure that \\((\\alpha, \\beta)\\) criterion holds with probability \\(0.9\\), we need to run \\(D^2\\) sampling \\(\\log_2\\left(\\frac{1}{1-0.9}\\right) \\approx 4\\) times and select the best clustering. The best clusturing is defined as minimum value of \\(cost(X,Q^*_B)\\). cost_QB = np.inf for trial in range(4): np.random.seed(trial) fst_idx = np.random.choice(N) # Choosing first center randomly B = [] # Approximate cluster centers B.append(c4X[fst_idx]) for choice in range(K-1): # Choice of remaining K-1 centers proba = np.square(cdist(c4X, np.array(B))).min(axis=1) norm_proba = proba/np.sum(proba) idx = np.random.choice(N, p=norm_proba) B.append(c4X[idx,:]) tmp_cost = np.square(cdist(c4X, np.array(B)).min(axis=1)).sum()/N if tmp_cost&lt;cost_QB: cost_QB = tmp_cost B_star = B.copy() As per the \\((\\alpha, \\beta)\\) criterion, \\(cost(X,Q^*_B) \\le \\alpha\\cdot cost(X,Q^*_X)\\), where \\(\\alpha=16(\\log_2K+2)cost(X,Q^*_X)\\). Thus, ratio \\(R(Q^*_B, Q^*_X) = \\frac{cost(X,Q^*_B)}{cost(X,Q^*_X)} \\le \\alpha\\). Lower the ratio, better the approximation we have. Let us see how much ratio we get experimentally. alpha = 16*(np.log2(K) + 2) alpha_R = cost_QB/cost_QX; \\(\\alpha=\\) 64 and experimental ratio is 1.59. The experimental ratio is much lower than the upper limit, which is expected given the well-separated clusters and small data. Let us visualize the approximate centers \\(Q^*_B\\). Q_B = np.array(B_star) plt.scatter(c4X[:,0], c4X[:,1], c=c4y, alpha=0.5); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, c=&#39;green&#39;, label=&#39;$Q^*_X$&#39;); plt.scatter(Q_B[:,0], Q_B[:,1], s=200, marker=&#39;d&#39;, c=&#39;red&#39;, label=&#39;$Q^*_B$&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() Now, we will perform the importance sampling to select the coreset points. Before that, following are the essential variables need to be computed to calculate sensitivity and probability for the importance sampling. B_y = cdist(c4X, Q_B).argmin(axis=1); # Cluster labels according to Q_B cost_QB = np.square(cdist(c4X, Q_B)).min(axis=1); # Cost(X, Q_B) mean_cost_QB = np.mean(cost_QB); cost_QB_cluster = [cost_QB[B_y==C].mean() for C in range(K)]; # Cost of each cluster cluster_n = pd.Series(B_y).sort_index().value_counts().values # Cardinality of each cluster S = 6*alpha + 4*K; # Total sensitivity We can calculate the sensitivity and probability for each data-point as following, # Sensitivity s_x = np.array([2*alpha*cost_QB[i]/mean_cost_QB +\\ 4*alpha*cost_QB_cluster[B_y[i]]/mean_cost_QB +\\ 4*N/cluster_n[B_y[i]] for i in range(N)]) # Probability q_x = s_x/S/N assert np.sum(q_x).round(2) == 1. q_x = q_x/q_x.sum() # Adjusting for numerical precision to make sum(q)=1 Let us visualize the probabilities of the data-points. map_ax = plt.scatter(c4X[:,0], c4X[:,1], c=q_x); plt.colorbar(map_ax); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We have completed all the pre-requisite steps for importance sampling. Now, let us perform the importance sampling. Initially, we will set the coreset size to only 10% of the total data-points. np.random.seed(0) C = int(0.1*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, we can run the Weighted KMeans algorithm Abras and Balları́n (2005) to get unbiased cluster centers \\(Q^*_C\\). After that, we can visually see the minimum number of samples for which the coreset property holds (when \\(Q^*_C\\) becomes a good approximation for \\(Q^*_X\\) according to definition of coresets). cost_QC = [] # cost(X, Q_C) for n_points in range(K, C+K): tmp_model = KMeans(n_clusters=K, random_state=0); tmp_model = tmp_model.fit(c4C[:n_points], sample_weight=Cw[:n_points]); Q_C = tmp_model.cluster_centers_ tmp_cost = np.square(cdist(c4X, Q_C)).min(axis=1).sum()/N cost_QC.append(tmp_cost) plt.plot(range(K, C+K), np.array(cost_QC)/cost_QX, &#39;o-&#39;, markersize=5); plt.hlines(1, *plt.xlim(), label=&#39;$1$&#39;); plt.hlines(1+3*epsilon, *plt.xlim(), label=&#39;$1+3 \\\\varepsilon $&#39;, color=&#39;red&#39;); plt.xlabel(&#39;Number of coreset points&#39;);plt.ylabel(&#39;$\\\\frac{cost(X, Q_C)}{cost(X, Q_X)}$&#39;); plot_essentials() We see that coreset property holds at a substantially smaller number of points. However, number of coreset points do not depend on number of datapoints \\(N\\), but it depends on number of clusters \\(K\\), dimension of data \\(d\\), \\(\\varepsilon\\) and minimum probability \\(\\delta\\) for coreset. The upper bound defined by Bachem, Lucic, and Krause (2017) suggests drawing nearly 2000 points for our current settings, but we can see that, in practice, coreset property holds with much lesser number of coreset points. Theoretical bound is given as the following, \\[ \\text{Number of coreset points } m = \\Omega\\left( \\frac{dK^3\\log(K)+K^2\\log(\\frac{1}{\\delta})}{\\varepsilon^2}\\right) \\] We will draw a coreset with size 20% of the original dataset and fix it as the coreset for further analysis. np.random.seed(0) C = int(0.2*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, let us visualize the cluster centers \\(Q^*_C\\) found by running KMeans on the coreset and cluster centers \\(Q^*_X\\) found by running KMeans on the original dataset (again, just to demonstrate). full_model = KMeans(n_clusters=K, random_state=0) full_model = full_model.fit(c4X) cost_QX = -full_model.score(c4X) # cost(X, Q_X) (score is opposite of cost) Q_X = full_model.cluster_centers_ X_labels = full_model.predict(c4X) coreset_model = KMeans(n_clusters=K, random_state=0) coreset_model = coreset_model.fit(c4C, sample_weight=Cw) cost_QC = -coreset_model.score(c4X) # cost(X, Q_C) Q_C = coreset_model.cluster_centers_ C_labels = coreset_model.predict(c4X) colors1 = [&#39;tab:blue&#39;, &#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;m&#39;] mapper1 = lambda x: [colors1[i] for i in x] colors2 = [&#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;tab:blue&#39;, &#39;m&#39;] mapper2 = lambda x: [colors2[i] for i in x] plt.scatter(c4X[:,0], c4X[:,1], c=mapper1(X_labels), marker=&#39;&gt;&#39;, label=&#39;clusters of $Q_X$&#39;, s=50); plt.scatter(c4X[:,0], c4X[:,1], c=mapper2(C_labels), marker=&#39;&lt;&#39;, label=&#39;clusters of $Q_C$&#39;, s=50); plt.scatter(Q_X[:,0], Q_X[:,1], c=&#39;k&#39;, marker=&#39;&gt;&#39;, s=100, label=&#39;$Q_X$&#39;); plt.scatter(Q_C[:,0], Q_C[:,1], c=&#39;k&#39;, marker=&#39;&lt;&#39;, s=100, label=&#39;$Q_C$&#39;); plot_essentials() \\(cost(X, Q^*_X)=\\) 116 \\(cost(X, Q^*_C)=\\) 156 \\(R=\\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\\) 1.343 \\(&lt; \\frac{1+\\varepsilon}{1-\\varepsilon}=\\) 1.857 We can see that \\(Q^*_C\\) is able to approximate the \\(Q^*_X\\) efficiently as per our requirement of \\(\\varepsilon\\)-coreset. "],["references.html", "References", " References Abras, Guillermo N, and Virginia Laura Balları́n. 2005. “A Weighted k-Means Algorithm Applied to Brain Tissue Classification.” Journal of Computer Science &amp; Technology 5. Bachem, Olivier, Mario Lucic, and Andreas Krause. 2017. “Practical Coreset Constructions for Machine Learning.” http://arxiv.org/abs/1703.06476. "]]
