[["index.html", "IN792: Coresets Coresets: 1 Preface", " IN792: Coresets Zeel B Patel 2021-05-06 Coresets: 1 Preface This report is generated for work done in the IN-792 project with Prof. Anirban Dasgupta in Semester-II of the academic year 2020-2021. Please go to GitHub discussions for any Q&amp;A, suggestions, or error reporting. "],["coresets.html", "Coresets: 2 Coresets 2.1 What are coresets?", " Coresets: 2 Coresets 2.1 What are coresets? An accurate definition of coresets is the following given by Bachem, Lucic, and Krause (2017), Coresets are small, weighted summaries of large data sets such that solutions found on the summary itself are provably competitive with solutions found on the entire data set. More intuitively, coresets are small subsets of the larger data (with weights associated with each data point) that can represent the original data with a good approximation on a particular model/algorithm in terms of cost. Coresets are extremely useful for models that do not scale well with data. If a model can not be optimized further on time complexity, using coresets is a realistic option to cope up with training time. Theoretical guarantee is a unique property about coresets over other approximation techniques, making it robust for practical usage. In the next chapter, we shall see the introduction to coresets for the KMeans clustering algorithm. "],["coresets-for-kmeans-theory.html", "Coresets: 3 Coresets for KMeans (Theory) 3.1 Problem formulation 3.2 Uniform sampling 3.3 Importance sampling", " Coresets: 3 Coresets for KMeans (Theory) This chapter’s motive is to highlight the key ideas behind KMeans coreset construction and give intuitive proofs for some of the ideas. For more details, please refer to Bachem, Lucic, and Krause (2017). 3.1 Problem formulation In the KMeans clustering problem, we aim to cluster the dataset \\(X \\in R^d\\) of cardinality \\(n\\) in \\(K\\) seperate clusters. If \\(Q\\) is a set of cluster centers for KMeans in this problem, we can define the cost as the following, \\[ cost(X, Q) = \\frac{1}{n}\\sum\\limits_{x\\in X}f_Q(x) = \\frac{1}{n}\\sum\\limits_{x\\in X}\\min\\limits_{q\\in Q}||x-q||^2_2 \\] If \\(C\\) of cardinality \\(n&#39;\\) is a weighted coreset constructed from dataset \\(X\\), \\(n&#39;&lt;&lt;n\\) is our desired property. Note that \\(c \\in C\\) are i.i.d. samples. \\(C\\) is a valid \\(\\varepsilon\\)-coreset of \\(X\\) if the following property holds for any \\(Q\\) with high probability \\(\\delta\\) (\\(\\delta\\) can be quantified), \\[ |cost(X, Q) - cost(C,Q)| \\le \\varepsilon cost(X,Q) \\] There is a derived result of the above theorem, which is more useful in practice. If \\(Q^*_X\\) is optimal cluster centers obtained by executing KMeans on full dataset \\(X\\) and \\(Q^*_C\\) is optimal cluster centers obtained by executing KMeans on coreset \\(C\\), the following property holds with high probability, \\[ cost(X,Q^*_X) \\le cost(X, Q^*_C) \\le \\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X) \\] Note that, in practice, computing \\(Q^*_X\\) is not feasible. Thus we are willing to bear a higher cost up to \\(\\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X)\\) at the benefit of reduced computational time. This property ensures that cost on the coreset stays within the defined upper bound without actually computing \\(Q^*_X\\) (which is obvious but just being explicit). We need to find a method to construct the coreset \\(C\\) such that the above properties hold with high probability (yes, nothing is deterministic in the probabilistic world). Let us try a naive method first. 3.2 Uniform sampling The first idea that comes to mind for coreset construction is uniform random sampling. We will mathematically see if a coreset constructed by uniform random sampling is useful or not. First we need to verify if \\(cost(C, Q)\\) is an unbiased estimator of \\(cost(X, Q)\\). Intuitively, we can say that, over multiple choices of coresets \\(C\\), we expect value of \\(cost(C, Q)\\) to stay closer to \\(cost(X, Q)\\) and if we take \\(\\mathbb{E}_C(cost(C,Q))\\) over a long run, it should converge to \\(cost(X, Q)\\). Now, let us prove the same as following, \\[\\begin{aligned} \\mathbb{E}_C(cost(C, Q)) &amp;= \\mathbb{E}_C\\left(\\frac{1}{m}\\sum\\limits_{c \\in C}f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\mathbb{E}(f_Q(c))\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}\\frac{1}{n}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}cost(X,Q)\\\\ &amp;= cost(X, Q) \\end{aligned}\\] We saw that \\(cost(C,Q)\\) converges to \\(cost(X,Q)\\) in expectation. But, we need to minimize \\(Var(cost(C,Q))\\) as well, so that \\(cost(C,Q)\\) is as close as possible to \\(cost(X,Q)\\). \\[\\begin{aligned} Var(cost(C,Q)) &amp;= Var(\\frac{1}{m}\\sum\\limits_{c \\in C} f_Q(c))\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c \\in C}Var(f_Q(c))\\\\ &amp;\\le \\frac{1}{m^2}\\sum\\limits_{c \\in C}\\mathbb{E}(f_Q(c)^2) = \\frac{1}{m^2}\\sum\\limits_{c \\in C}\\frac{1}{n}\\sum\\limits_{x \\in X}f_Q(x)^2\\\\ &amp;\\le \\frac{1}{nm^2}\\sum\\limits_{c \\in C}\\left(\\sum\\limits_{x \\in X}f_Q(x)\\right)^2\\\\ &amp;= \\frac{1}{nm^2}\\sum\\limits_{c \\in C}\\left(n\\cdot cost(X,Q)\\right)^2\\\\ &amp;= \\frac{n}{m^2}\\sum\\limits_{c \\in C}cost(X,Q)^2\\\\ &amp;= \\frac{n}{m}cost(X,Q)^2 \\end{aligned}\\] We can see that \\(m \\to \\infty\\) then \\(Var(cost(C,Q)) \\to 0\\). Let us get an estimate of \\(m\\) using Chebyshev’s inequality. \\[ P\\left(|cost(C, Q) - cost(X, Q)| \\ge k\\sqrt{\\frac{n}{m}}cost(X,Q)\\right) \\le \\frac{1}{k^2} \\] Substituting \\(k\\sqrt{\\frac{n}{m}} = \\varepsilon \\to k = \\varepsilon\\sqrt{\\frac{m}{n}}\\). \\[ \\delta = P\\left(|cost(C, Q) - cost(X, Q)| \\ge \\varepsilon \\cdot cost(X,Q)\\right) \\le \\frac{n}{\\varepsilon^2m}\\\\ \\] The above expression suggests that if we are interested in \\(\\varepsilon\\)-coreset with atleast (\\(1-\\delta\\)) probability, we need to discard all \\(m \\le \\frac{n}{\\varepsilon^2\\delta}\\). Thus we have \\(m \\ge \\frac{n}{\\varepsilon^2\\delta}\\). The same phenomenon is illustrated in the below plot, Looking at the numbers, to construct the coreset with atleast 70% probability for dataset of size \\(n=100\\), we need to sample atleast \\(m=\\frac{n}{\\varepsilon^2\\delta}=4000\\) points. This is clearly not useful. Also, note that \\(m \\propto n\\), so, coreset size will increase as \\(n\\) increases maintaining \\(m&gt;&gt;n\\). Thus, this method fails to construct a coreset. One may want to have a look at an intuitive example given in Bachem, Lucic, and Krause (2017). We need to change our sampling strategy to build better coresets. Let us move to importance sampling now. 3.3 Importance sampling Let us say we sample each point \\(x\\) with probability \\(q(x)\\). In this process, we might not have \\(cost(C,Q)\\) as an unbiased estimator of \\(cost(X,Q)\\) anymore. Thus, we need to introduce weights \\(\\mu_C(c), c \\in C\\). We will use the weighted \\(cost(C, Q)\\) as defined below, \\[ cost(C, Q) = \\frac{1}{m}\\sum\\limits_{c \\in C} \\mu_C(c)f_Q(c) \\] Let us find out a value of \\(\\mu_C(c)\\) such that \\(\\mathbb{E}_C(cost(C,Q))\\) converges to \\(cost(X, Q)\\) \\[\\begin{aligned} \\mathbb{E}_C(cost(C,Q)) &amp;= \\mathbb{E}_C\\left(\\frac{1}{m}\\sum\\limits_{c \\in C} \\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\mathbb{E}(\\mu_C(c)f_Q(c))\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}q(x)\\mu_C(x)f_Q(x)\\\\ \\text{Now, we should take }\\mu_C(x) = \\frac{1}{nq(x)}\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}q(x)\\frac{1}{nq(x)}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\frac{1}{n}\\sum\\limits_{x \\in X}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}cost(X, Q)\\\\ &amp;= cost(X, Q) \\end{aligned}\\] Now, we shall calculate the \\(Var(C(C, Q))\\) and find the conditions that minimize it. \\[\\begin{aligned} Var(cost(C,Q)) &amp;= Var\\left(\\frac{1}{m}\\sum\\limits_{c\\in C}\\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}Var\\left(\\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\mathbb{E}\\left(\\left(\\mu_C(c)f_Q(c)\\right)^2\\right) - \\left(\\mathbb{E}\\left(\\mu_C(c)f_Q(c)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\sum\\limits_{x\\in X}q(x)\\left(\\mu_C(x)f_Q(x)\\right)^2 - \\left(\\sum\\limits_{x\\in X}\\left(q(x)\\mu_C(x)f_Q(x)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\sum\\limits_{x\\in X}q(x)\\left(\\frac{1}{nq(x)}f_Q(x)\\right)^2 - \\left(\\sum\\limits_{x\\in X}\\left(q(x)\\frac{1}{nq(x)}f_Q(x)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{n^2m}\\left(\\sum\\limits_{x\\in X}\\frac{(f_Q(x))^2}{q(x)} - \\left(cost(X,Q)\\right)^2\\right) \\end{aligned}\\] If we choose \\(q(x) = \\frac{f_Q(x)}{\\sum\\limits_{x \\in X}f_Q(x)}\\), \\(Var(cost(C,Q)) = 0\\). Intuitively it is sensible, because we give more priority to the points that individually contribute more in the total cost (\\(f_Q(x)\\) is high). We have done this calculation for a single set of clusters \\(Q\\), but we need to generalize this to any \\(Q\\). Note that currently, \\(q(x)\\) depends on \\(Q\\), which should be avoided for generality. To generalize importance sampling further, we need a concept called sensitivity, which is intuitively proportional to \\(q(x)\\) but is more general and does not depend on any particular \\(Q\\). 3.3.1 Sensitivity Sensitivity \\(\\sigma(x)\\) is defined as the following for all \\(Q, Q \\in \\mathcal{Q}\\), where \\(\\mathcal{Q}\\) is the set of all possible cluster centers (really? How do we find all possible cluster centers? We do not need to find them. We have other tricks to do that which is currently out of scope for this report. Please refer to Bachem, Lucic, and Krause (2017) later). \\[ \\sigma(x) = \\sup\\limits_{Q \\in \\mathcal{Q}}\\frac{f_Q(x)}{cost(X,Q)} \\] We can see that optimal \\(Q\\) might not be the same for all \\(x\\) and thus calculating \\(\\sigma(x)\\) might be another problem. But, we introduce a general upper bound \\(s(x)\\) on \\(\\sigma(x)\\). We have average sensitivity \\(S = \\frac{1}{n}\\sum\\limits_{x \\in X}s(x)\\). Now, we can modify our sampling distribution \\(q(x)\\) as following, \\[ q(x) = \\frac{1}{n}\\frac{s(x)}{S} = \\frac{s(x)}{\\sum\\limits_{x \\in X}s(x)} \\] Now, we try to get an estimate of a lower bound on \\(m\\). Consider the following function \\(g_Q(x)\\). \\[ g_Q(x) = \\frac{f_Q(x)}{n\\cdot cost(X,Q)}\\frac{1}{Sq(x)} \\] Using Hoeffding’s inequality, we have the following formula, \\[ P\\left(\\left|\\mathbb{E}(g_Q(x)) - \\frac{1}{m}\\sum\\limits_{x \\in X}g_Q(x)\\right| &gt; \\varepsilon&#39;\\right) \\le 2\\exp\\left(-2m\\varepsilon&#39;^2\\right) \\] One can verify that \\(|\\mathbb{E}(g_Q(x)) = \\frac{1}{S}\\) and \\(\\frac{1}{m}\\sum\\limits_{x \\in X}g_Q(x) = \\frac{cost(C,Q)}{Scost(X,Q)}\\). Using the same result, we get, \\[ P(\\left|cost(X,Q) - cost(X,Q)\\right| &gt; \\varepsilon&#39;Scost(X,Q)) \\le 2\\exp\\left(-2m\\varepsilon&#39;^2\\right) \\] Hence, we can say that if \\(C\\) is \\(\\varepsilon\\)-coreset of \\(X\\) with atleast (\\(1-\\delta\\)) probability, estimate of \\(m\\) is the following, \\[ m \\ge \\frac{S^2}{2\\varepsilon^2}\\log_e\\frac{2}{\\delta} \\] We can see that, \\(m \\propto S^2\\), in case we consider \\(s(x)=n\\) and so effectively \\(S=n\\), we have \\(m \\propto n^2\\) (which is worst than original dataset itself). But, we have other ways to create tighter bounds \\(s(x)\\) to build useful coresets. Note that, tighter the bound \\(s(x)&gt;\\sigma(x)\\), better coresets we get. 3.3.2 Rough approximation Using the rough approximation techniques, we can find a theoretically bounded approximation of \\(cost(X, Q)\\) with much lesser computational power. One of the approaches used here is \\((\\alpha, \\beta)\\) bi-criterion approximation. \\((\\alpha, \\beta)\\) approximation states that, for a set of cluster centers \\(Q_B\\) of cardinality \\(|\\beta K|\\), the following property holds, \\[ cost(X, Q_B) &lt; \\alpha\\;cost(X, Q^*_X) \\] Arthur and Vassilvitskii (2007) propose an efficient algorithm that holds \\((\\alpha, \\beta)\\) bi-criterion. Traditionally, the algorithm is known as the \\(D^2\\) sampling algorithm, which is given in Section 6, Arthur and Vassilvitskii (2007) also shows that the following result holds with at least \\(\\delta\\) probability when \\(Q^*_B\\) is best cluster centers selected by running the \\(D^2\\) sampling algorithm \\(\\log_2\\frac{1}{1-\\delta}\\) times. \\[ cost(X, Q^*_B) \\le 16(\\log_2K+2)cost(X, Q^*_X) \\] 3.3.3 Bounding sensitivity Now, the final Lemma combines all the concepts we have seen thus far and gives a tighter bound on sensitivity \\(s(x)\\). For each point \\(x \\in X\\), we define a set of points \\(X_x\\) that share a common cluster center \\(b_x \\in Q^*_B\\), then the sensitivity \\(\\sigma(x)\\) is bounded by, \\[\\begin{aligned} \\bar{c}_B &amp;= \\frac{1}{n}\\sum\\limits_{x \\in X}d(x, b_x)\\\\ s(x) &amp;= \\frac{2 \\alpha\\;d(x, b_x)^2}{\\bar{c}_B} + \\frac{4\\alpha\\;\\sum\\limits_{x \\in X_x}d(x, b_x)}{|X_x|\\bar{c}_B} + \\frac{4n}{|X_x|}\\\\ S &amp;= 6\\alpha + 4K \\end{aligned}\\] This result also holds for any \\(Q \\in \\mathcal{Q}\\). Please refer to Bachem, Lucic, and Krause (2017) for the proof and subtle details. 3.3.4 Algorithm to create KMeans coreset Now, combining all the steps, the algorithm to generate KMeans clustering can be given as the following, Algorithm 2: Coreset construction for KMeans clustering Require: dataset \\(X\\), number of clusters \\(K\\). Run \\(D^2\\) algorithm multiple times on the original dataset \\(X\\) to get \\(Q^*_B\\). calculate sensitivity scores \\(s(x)\\) and effectively \\(S\\) calculate probability distribution \\(q(x) = \\frac{s(x)}{nS}\\) sample a set of points \\(C\\) from \\(X\\) using \\(q(x)\\) until the coreset property is satisfied. Run Weighted KMeans algorithm on \\(C\\) considering the weights \\(\\mu_C(x) = \\frac{1}{q(x)}\\) (not \\(\\frac{1}{nq(x)}\\) because \\(n\\) will be anyway considered in the average cost) Resultant cluster centers set \\(Q^*_C\\) is an approximated set of cluster centers theoretically closer to \\(Q^*_X\\). In the next chapter, we will implement the importance sampling in coresets for KMeans clustering step by step. "],["KMP.html", "Coresets: 4 Coresets for KMeans (Practical) 4.1 Pseudo-data with 4 clusters 4.2 Pseudo-dataset with 100 clusters and 10000 data-points", " Coresets: 4 Coresets for KMeans (Practical) In this practical, we will try to create an \\(\\varepsilon\\)-coreset (\\(\\varepsilon=0.3\\)) for KMeans clustering. Let us import some packages and set up a few useful functions, from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from scipy.spatial.distance import cdist, pdist import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) rc(&#39;font&#39;, size=16) rc(&#39;text&#39;, usetex=True) def plot_essentials(): # essential code for every plot hand, labs = plt.gca().get_legend_handles_labels() if len(hand)&gt;0: plt.legend(hand, labs); plt.tight_layout(); plt.show() plt.close() 4.1 Pseudo-data with 4 clusters Now, we generate a pseudo-dataset with 4 clusters. N = 100 K = 4 d = 2 epsilon = 0.3 c4X, c4y = make_blobs(n_samples=N, centers=K, n_features=d, random_state=0, cluster_std=0.8) plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We run the KMeans algorithm on the full dataset to calculate \\(cost(X, Q^*_X)\\). Note that, in practice, this is infeasible because of superlinear time-complexity, but we perform this step to have a comparison between \\(cost(X, Q^*_X)\\) and \\(cost(X, Q^*_C)\\). full_model = KMeans(n_clusters=K, random_state=0) full_model.fit(c4X); cost_QX = full_model.inertia_/N Optimal cost on the full dataset is \\(cost(X, Q^*_X)=\\) 1.16. Q_X = full_model.cluster_centers_ plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, label=&#39;$Q^*_X$&#39;, c=&#39;green&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plt.title(&#39;Optimal cluster centers on the original dataset&#39;); plot_essentials() The next step is running the \\(D^2\\) sampling algorithm on the full dataset to get the approximate centers \\(Q^*_B\\). To ensure that \\((\\alpha, \\beta)\\) criterion holds with probability \\(0.9\\), we need to run \\(D^2\\) sampling \\(\\log_2\\left(\\frac{1}{1-0.9}\\right) \\approx 4\\) times and select the best clustering (least cost). cost_QB = np.inf for trial in range(4): np.random.seed(trial) fst_idx = np.random.choice(N) # Choosing first center randomly B = [] # Approximate cluster centers B.append(c4X[fst_idx]) for choice in range(K-1): # Choice of remaining K-1 centers proba = np.square(cdist(c4X, np.array(B))).min(axis=1) norm_proba = proba/np.sum(proba) idx = np.random.choice(N, p=norm_proba) B.append(c4X[idx,:]) tmp_cost = np.square(cdist(c4X, np.array(B)).min(axis=1)).sum()/N if tmp_cost&lt;cost_QB: cost_QB = tmp_cost B_star = B.copy() As per the \\((\\alpha, \\beta)\\) criterion, \\(cost(X,Q^*_B) \\le \\alpha\\cdot cost(X,Q^*_X)\\), where \\(\\alpha=16(\\log_2K+2)cost(X,Q^*_X)\\). Thus, ratio \\(R(Q^*_B, Q^*_X) = \\frac{cost(X,Q^*_B)}{cost(X,Q^*_X)} \\le \\alpha\\). Lower the ratio, better the approximation we have. Let us see how much ratio we get experimentally. alpha = 16*(np.log2(K) + 2) alpha_R = cost_QB/cost_QX; \\(\\alpha=\\) 64 and experimental ratio is 1.59. The experimental ratio is much lower than the upper limit, which is expected given the well-separated clusters and small data. Let us visualize the approximate centers \\(Q^*_B\\). Q_B = np.array(B_star) plt.scatter(c4X[:,0], c4X[:,1], c=c4y, alpha=0.5); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, c=&#39;green&#39;, label=&#39;$Q^*_X$&#39;); plt.scatter(Q_B[:,0], Q_B[:,1], s=200, marker=&#39;d&#39;, c=&#39;red&#39;, label=&#39;$Q^*_B$&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() Now, we will perform the importance sampling to select the coreset points. The following are the essential variables that we need to compute to calculate sensitivity and probability for the importance sampling. B_y = cdist(c4X, Q_B).argmin(axis=1); # Cluster labels according to Q_B cost_QB = np.square(cdist(c4X, Q_B)).min(axis=1); # Cost(X, Q_B) mean_cost_QB = np.mean(cost_QB); cost_QB_cluster = [cost_QB[B_y==C].mean() for C in range(K)]; # Cost of each cluster cluster_n = pd.Series(B_y).sort_index().value_counts().values # Cardinality of each cluster S = 6*alpha + 4*K; # Total sensitivity We can calculate the sensitivity and probability for each data-point as following, # Sensitivity s_x = np.array([2*alpha*cost_QB[i]/mean_cost_QB +\\ 4*alpha*cost_QB_cluster[B_y[i]]/mean_cost_QB +\\ 4*N/cluster_n[B_y[i]] for i in range(N)]) # Probability q_x = s_x/S/N assert np.sum(q_x).round(2) == 1. q_x = q_x/q_x.sum() # Adjusting for numerical precision to make sum(q)=1 Let us visualize the probabilities of the data-points. map_ax = plt.scatter(c4X[:,0], c4X[:,1], c=q_x); plt.colorbar(map_ax); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We have completed all the pre-requisite steps for importance sampling. Now, let us perform the importance sampling. Initially, we will set the coreset size to only 10% of the total data-points. np.random.seed(0) C = int(0.1*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, we can run the Weighted KMeans algorithm Abras and Balları́n (2005) to get unbiased cluster centers \\(Q^*_C\\). After that, we can visually see the minimum number of samples for which the coreset property holds (when \\(Q^*_C\\) becomes a good approximation for \\(Q^*_X\\) according to the definition of coresets). cost_QC = [] # cost(X, Q_C) cost_QC_biased = [] for n_points in range(K, C+K): # Unbiased estimate tmp_model = KMeans(n_clusters=K, random_state=0); tmp_model = tmp_model.fit(c4C[:n_points], sample_weight=Cw[:n_points]); Q_C = tmp_model.cluster_centers_ tmp_cost = np.square(cdist(c4X, Q_C)).min(axis=1).sum()/N cost_QC.append(tmp_cost) # Biased estimate tmp_model_biased = KMeans(n_clusters=K, random_state=0); tmp_model_biased = tmp_model_biased.fit(c4C[:n_points]); Q_C_biased = tmp_model_biased.cluster_centers_ tmp_cost_biased = np.square(cdist(c4X, Q_C_biased)).min(axis=1).sum()/N cost_QC_biased.append(tmp_cost_biased) plt.plot(range(K, C+K), np.array(cost_QC)/cost_QX, &#39;o-&#39;, markersize=5, label=&#39;unbiased ratio&#39;); plt.plot(range(K, C+K), np.array(cost_QC_biased)/cost_QX, &#39;o-&#39;, markersize=5, label=&#39;biased ratio&#39;); plt.hlines(1, *plt.xlim(), label=&#39;$1$&#39;); plt.hlines(1+3*epsilon, *plt.xlim(), label=&#39;$1+3 \\\\varepsilon $&#39;, color=&#39;red&#39;); plt.xlabel(&#39;Number of coreset points&#39;);plt.ylabel(&#39;$\\\\frac{cost(X, Q_C)}{cost(X, Q_X)}$&#39;); plot_essentials() We see that the coreset property holds at a substantially smaller number of points. However, the number of coreset points does not depend on the number of data-points \\(N\\), but it depends on the number of clusters \\(K\\), dimension of data \\(d\\), \\(\\varepsilon\\) and minimum probability \\(\\delta\\) for coreset. The upper bound defined by Bachem, Lucic, and Krause (2017) suggests drawing nearly 2000 points for our current settings, but we can see that, in practice, coreset property holds with a much lesser number of coreset points. Theoretical bound is given as the following, \\[ \\text{Number of coreset points } m = \\Omega\\left( \\frac{dK^3\\log(K)+K^2\\log(\\frac{1}{\\delta})}{\\varepsilon^2}\\right) \\] We will draw a coreset with a 20% size of the original dataset and fix it as the coreset for further analysis. np.random.seed(0) C = int(0.2*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, let us visualize the cluster centers \\(Q^*_C\\) found by running KMeans on the coreset and cluster centers \\(Q^*_X\\) found by running KMeans on the original dataset (again, just to demonstrate). full_model = KMeans(n_clusters=K, random_state=0) full_model = full_model.fit(c4X) cost_QX = -full_model.score(c4X) # cost(X, Q_X) (score is opposite of cost) Q_X = full_model.cluster_centers_ X_labels = full_model.predict(c4X) coreset_model = KMeans(n_clusters=K, random_state=0) coreset_model = coreset_model.fit(c4C, sample_weight=Cw) cost_QC = -coreset_model.score(c4X) # cost(X, Q_C) Q_C = coreset_model.cluster_centers_ C_labels = coreset_model.predict(c4X) colors1 = [&#39;tab:blue&#39;, &#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;m&#39;] mapper1 = lambda x: [colors1[i] for i in x] colors2 = [&#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;tab:blue&#39;, &#39;m&#39;] mapper2 = lambda x: [colors2[i] for i in x] plt.scatter(c4X[:,0], c4X[:,1], c=mapper1(X_labels), marker=&#39;&gt;&#39;, label=&#39;clusters of $Q^*_X$&#39;, s=50); plt.scatter(c4X[:,0], c4X[:,1], c=mapper2(C_labels), marker=&#39;&lt;&#39;, label=&#39;clusters of $Q^*_C$&#39;, s=50); plt.scatter(Q_X[:,0], Q_X[:,1], c=&#39;k&#39;, marker=&#39;&gt;&#39;, s=100, label=&#39;$Q^*_X$&#39;); plt.scatter(Q_C[:,0], Q_C[:,1], c=&#39;k&#39;, marker=&#39;&lt;&#39;, s=100, label=&#39;$Q^*_C$&#39;); plot_essentials() \\(cost(X, Q^*_X)=\\) 116 \\(cost(X, Q^*_C)=\\) 156 \\(R=\\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\\) 1.343 \\(&lt; \\frac{1+\\varepsilon}{1-\\varepsilon}=\\) 1.857 We can see that \\(Q^*_C\\) can approximate the \\(Q^*_X\\) efficiently as per our requirement of \\(\\varepsilon\\)-coreset. 4.2 Pseudo-dataset with 100 clusters and 10000 data-points Let us try the coreset construction on a more extensive dataset. First, we will generate a dataset with 100 clusters and 10000 points. I have hidden the code for better visuals and to avoid redundancy. Now, let us check the actual fit with vanilla KMeans algorithm, Now, we find approximate centers \\(Q^*_B\\) with \\(D^2\\) sampling, \\(\\alpha=\\) 138.3 and experimental ratio is 1.8. Let us visualize the probability distribution over all data points. Now, we visualize comparison between \\(cost(C,Q)\\) and \\(cost(X,Q)\\) as we increase \\(m\\). We see that \\(cost(C, Q)\\) starts satisfying coreset property well below 10% of the total dataset. Let us fix the coreset size at 10% of the entire dataset and check the effect on the coreset property. \\(cost(X, Q^*_X)=\\) 1585 \\(cost(X, Q^*_C)=\\) 1876 \\(R=\\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\\) 1.183 \\(&lt; \\frac{1+\\varepsilon}{1-\\varepsilon}=\\) 1.857 We can see that \\(Q^*_C\\) can approximate the \\(Q^*_X\\) efficiently as per our requirement of \\(\\varepsilon\\)-coreset. "],["coresets-for-linear-regression.html", "Coresets: 5 Coresets for Linear Regression", " Coresets: 5 Coresets for Linear Regression We will see one of the methods to select coreset points for linear regression. We calculate “Ridge leverage scores” to create a sampling distribution for coreset selection. The process to calculate sampling distribution \\(p(X)\\) is as the following. \\(X_* = \\begin{bmatrix}X \\\\ \\lambda I_d\\end{bmatrix}\\) \\(U \\Sigma V^T = X_*\\) \\(\\mathbf{p}(X) = ||U(0:n, :)||_2^2\\) Now, let us implement this in a python function. def get_proba(x, lmd): x_list = [np.ones(x.shape[0]), x.ravel()] x_extra = np.vstack(x_list).T A = np.vstack([x_extra, np.eye(x_extra.shape[1])*np.sqrt(lmd)]) U, S, V = np.linalg.svd(A, full_matrices=False) U1 = U[:x.shape[0], :] proba = np.square(U1).sum(axis=1)/np.square(U1).sum(axis=1).sum() return proba We will sample the coresets for "],["references.html", "References", " References "],["Dsqr.html", "Coresets: 6 Appendix 1: D\\(^2\\) Sampling", " Coresets: 6 Appendix 1: D\\(^2\\) Sampling The following algorithm is known as D\\(^2\\) sampling. It was proposed by Arthur and Vassilvitskii (2007) to improve the initialization of KMeans algorithm. We also leverage the same algorithm in coreset construction of KMeans algorithm (Section ??-4. Algorithm 1: \\(D^2\\) sampling Require: dataset \\(X\\), number of clusters \\(K\\). Sample \\(x\\) from \\(X\\) uniform randomly or preavailable weights. set \\(Q_B=\\{x\\}\\) for i \\(\\to\\) \\(2, 3, ..., K\\) do sample \\(x\\) from \\(X\\) with probability \\(p(x) = \\frac{d(x, Q_B)^2}{\\sum\\limits_{x&#39; \\in X}d(x&#39;, Q_B)^2}\\) and add \\(x\\) to \\(Q_B\\). return \\(Q_B\\) Intuitively, this algorithm tries to select centers which are far away from the already selected centers. Let us try D\\(^2\\) sampling on dummy data. from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from scipy.spatial.distance import cdist, pdist import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import rc from matplotlib.animation import FuncAnimation import warnings warnings.filterwarnings(&#39;ignore&#39;) rc(&#39;font&#39;, size=16) rc(&#39;text&#39;, usetex=True) def plot_essentials(): # essential code for every plot hand, labs = plt.gca().get_legend_handles_labels() if len(hand)&gt;0: plt.legend(hand, labs); plt.tight_layout(); plt.show() plt.close() Genereting pseudo-data, X, y = make_blobs(n_samples=200, n_features=2, centers=8, random_state=1, cluster_std=0.5) plt.scatter(X[:,0], X[:,1], c=y); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plt.title(&#39;Pseudo-data with 8 clusters&#39;); plot_essentials(); Now we visualize the D\\(^2\\) algorithm, np.random.seed(0) K = 8 N = len(X) first_idx = np.random.choice(N) # Choosing first center randomly B = [] # Approximate cluster centers B.append(X[first_idx]) for choice in range(K-1): # Choice of remaining K-1 centers proba = np.square(cdist(X, np.array(B))).min(axis=1) norm_proba = proba/np.sum(proba) idx = np.random.choice(N, p=norm_proba) B.append(X[idx,:]) B = np.array(B) # Plotting fig, ax = plt.subplots(); def update(i): ax.cla(); ax.scatter(X[:,0], X[:,1], c=y, label=&#39;Data points&#39;); ax.scatter(B[:i,0], B[:i,1], c=&#39;g&#39;, marker=&#39;s&#39;, s=50, label=&#39;Selected centers&#39;); ax.legend(); fig.tight_layout(); plt.close(); anim = FuncAnimation(fig, update, frames=range(8)); rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim.to_html5_video() ## &#39;&lt;video width=&quot;700&quot; height=&quot;500&quot; controls autoplay loop&gt;\\n &lt;source type=&quot;video/mp4&quot; src=&quot;data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAABMFm1kYXQAAAKuBgX//6rcRem9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\\nYXZmNTcuODMuMTAw\\n&quot;&gt;\\n Your browser does not support the video tag.\\n&lt;/video&gt;&#39; Abras, Guillermo N, and Virginia Laura Balları́n. 2005. “A Weighted k-Means Algorithm Applied to Brain Tissue Classification.” Journal of Computer Science &amp; Technology 5. Arthur, David, and Sergei Vassilvitskii. 2007. “K-Means++: The Advantages of Careful Seeding.” In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, 1027–35. SODA ’07. USA: Society for Industrial; Applied Mathematics. Bachem, Olivier, Mario Lucic, and Andreas Krause. 2017. “Practical Coreset Constructions for Machine Learning.” http://arxiv.org/abs/1703.06476. "]]
