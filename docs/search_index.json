[["index.html", "IN792: Coresets Coresets: 1 Preface", " IN792: Coresets Zeel B Patel 2021-04-02 Coresets: 1 Preface This is a report generated for work done in IN-792 project with Prof. Anirban Dasgupta in Semester-II of academic year 2020-2021. For any Q&amp;A, suggestions, error reporting, please go to our GitHub discussions. "],["coresets.html", "Coresets: 2 Coresets 2.1 What are coresets?", " Coresets: 2 Coresets 2.1 What are coresets? A well-explained definition of coresets is the following by Bachem, Lucic, and Krause (2017), Coresets are small, weighted summaries of large data sets such that solutions found on the summary itself are provably competitive with solutions found on the full data set. More intuitively, a coreset is a small sample of the larger data (with weights associated with each data point) that can represent the original data with a good approximation on a particular model/algorithm. Coresets are extremely useful for models that do not scale well with data. If a model can not be optimized further on time-complexity, using coresets is natural option to cope up with training time. Theoretical guarantee is a unique property about coresets over other approximation techniques, which makes it robust for practical usage. In next chapter we shall see the introduction to coresets for the KMeans clustering algorithm. "],["coresets-for-kmeans-theory.html", "Coresets: 3 Coresets for KMeans (Theory) 3.1 Problem formulation 3.2 Uniform sampling 3.3 Importance sampling", " Coresets: 3 Coresets for KMeans (Theory) Motive behind this chapter is to highlight the key ideas behind KMeans coreset construction and to give intuitive proofs for some of the ideas. For more details, please refer to Bachem, Lucic, and Krause (2017). 3.1 Problem formulation In the KMeans clustering problem, we aim to cluster the dataset \\(X \\in R^d\\) of cardinality \\(n\\) in \\(K\\) seperate clusters. If \\(Q\\) is a set of cluster centers for KMeans in this problem, we can define the cost as the following, \\[ cost(X, Q) = \\frac{1}{n}\\sum\\limits_{x\\in X}f_Q(x) = \\frac{1}{n}\\sum\\limits_{x\\in X}\\min\\limits_{q\\in Q}||x-q||^2_2 \\] If \\(C\\) of cardinality \\(n&#39;\\) is a weighted coreset constructed from dataset \\(X\\), \\(n&#39;&lt;&lt;n\\) is our desired property. Note that \\(c \\in C\\) are i.i.d. samples. \\(C\\) is a valid \\(\\varepsilon\\)-coreset of \\(X\\) if the following property holds for any \\(Q\\) with high probability \\(\\delta\\) (\\(\\delta\\) can be quantified), \\[ |cost(X, Q) - cost(C,Q)| \\le \\varepsilon cost(X,Q) \\] There is a consequence result of the above theorem which is more useful in practice. If \\(Q^*_X\\) is optimal cluster centers obtained by executing KMeans on full dataset \\(X\\) and \\(Q^*_C\\) is optimal cluster centers obtained by executing KMeans on coreset \\(C\\), the following property holds with high probability, \\[ cost(X,Q^*_X) \\le cost(X, Q^*_C) \\le \\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X) \\] Note that, in practice, computing \\(Q^*_X\\) is not feasible thus we are willing bear a higher cost upto \\(\\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X)\\) at benefit of reduced computational time. This property ensures that cost on the coreset stays within the defined upper bound without actually need of computing \\(Q^*_X\\) (which is obvious but just being explicit). Now, the question is how to construct the coreset \\(C\\) such that above properties hold with high probability (yes, nothing is deterministic in the probabilistic world). Let us try a naive method first. 3.2 Uniform sampling An immediate idea for coreset construction is uniform random sampling. We will mathematically see if a coreset constructed by uniform random sampling is useful or not. First we need to verify if \\(cost(C, Q)\\) is an unbiased estimator of \\(cost(X, Q)\\). Intuitively, we can say that, over multiple choices of coresets \\(C\\), we expect value of \\(cost(C, Q)\\) to stay closer to \\(cost(X, Q)\\) and if we take \\(\\mathbb{E}_C(cost(C,Q))\\) over a long run, it should converge to \\(cost(X, Q)\\). Now, let us prove the same as following, \\[\\begin{aligned} \\mathbb{E}_C(cost(C, Q)) &amp;= \\mathbb{E}_C\\left(\\frac{1}{m}\\sum\\limits_{c \\in C}f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\mathbb{E}(f_Q(c))\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}\\frac{1}{n}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}cost(X,Q)\\\\ &amp;= cost(X, Q) \\end{aligned}\\] We saw that \\(cost(C,Q)\\) converges to \\(cost(X,Q)\\) in expectation. But, we need to minimize \\(Var(cost(C,Q))\\) as well, so that \\(cost(C,Q)\\) is as close as possible to \\(cost(X,Q)\\). \\[\\begin{aligned} Var(cost(C,Q)) &amp;= Var(\\frac{1}{m}\\sum\\limits_{c \\in C} f_Q(c))\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c \\in C}Var(f_Q(c))\\\\ &amp;\\le \\frac{1}{m^2}\\sum\\limits_{c \\in C}\\mathbb{E}(f_Q(c)^2) = \\frac{1}{m^2}\\sum\\limits_{c \\in C}\\frac{1}{n}\\sum\\limits_{x \\in X}f_Q(x)^2\\\\ &amp;\\le \\frac{1}{nm^2}\\sum\\limits_{c \\in C}\\left(\\sum\\limits_{x \\in X}f_Q(x)\\right)^2\\\\ &amp;= \\frac{1}{nm^2}\\sum\\limits_{c \\in C}\\left(n\\cdot cost(X,Q)\\right)^2\\\\ &amp;= \\frac{n}{m^2}\\sum\\limits_{c \\in C}cost(X,Q)^2\\\\ &amp;= \\frac{n}{m}cost(X,Q)^2 \\end{aligned}\\] We can see that \\(m \\to \\infty\\) then \\(Var(cost(C,Q)) \\to 0\\). Let us get an estimate of \\(m\\) using Chebyshev’s inequality. \\[ P\\left(|cost(C, Q) - cost(X, Q)| \\ge k\\sqrt{\\frac{n}{m}}cost(X,Q)\\right) \\le \\frac{1}{k^2} \\] Substituting \\(k\\sqrt{\\frac{n}{m}} = \\varepsilon \\to k = \\varepsilon\\sqrt{\\frac{m}{n}}\\). \\[ \\delta = P\\left(|cost(C, Q) - cost(X, Q)| \\ge \\varepsilon \\cdot cost(X,Q)\\right) \\le \\frac{n}{\\varepsilon^2m}\\\\ \\] The above expression suggests that if we are interested in \\(\\varepsilon\\)-coreset with atleast (\\(1-\\delta\\)) probability, we need to discard all \\(m \\le \\frac{n}{\\varepsilon^2\\delta}\\). Thus we have \\(m \\ge \\frac{n}{\\varepsilon^2\\delta}\\). The same phenomenon is illustrated in the below plot, Looking at the numbers, to construct the coreset with atleast 70% probability for dataset of size \\(n=100\\), we need to sample atleast \\(m=\\frac{n}{\\varepsilon^2\\delta}=4000\\) points. This is clearly not useful. Also, note that \\(m \\propto n\\), so, coreset size will increase as \\(n\\) increases maintaining \\(m&gt;&gt;n\\). Thus, this method fails to construct a coreset. One may want to have a look at an intuitive example given in Bachem, Lucic, and Krause (2017). We need to change our sampling strategy to construct better coresets. Let us move to importance sampling now. 3.3 Importance sampling Let us say we sample each point \\(x\\) with probability \\(q(x)\\). In this process, we might not have \\(cost(C,Q)\\) as an unbiased estimator of \\(cost(X,Q)\\) anymore. Thus, we need to introduce weights \\(\\mu_C(c), c \\in C\\). We will use the weighted \\(cost(C, Q)\\) as defined below, \\[ cost(C, Q) = \\frac{1}{m}\\sum\\limits_{c \\in C} \\mu_C(c)f_Q(c) \\] Let us find out a value of \\(\\mu_C(c)\\) such that \\(\\mathbb{E}_C(cost(C,Q))\\) converges to \\(cost(X, Q)\\) \\[\\begin{aligned} \\mathbb{E}_C(cost(C,Q)) &amp;= \\mathbb{E}_C\\left(\\frac{1}{m}\\sum\\limits_{c \\in C} \\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\mathbb{E}(\\mu_C(c)f_Q(c))\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}q(x)\\mu_C(x)f_Q(x)\\\\ \\text{Now, we should take }\\mu_C(x) = \\frac{1}{nq(x)}\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}q(x)\\frac{1}{nq(x)}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\frac{1}{n}\\sum\\limits_{x \\in X}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}cost(X, Q)\\\\ &amp;= cost(X, Q) \\end{aligned}\\] Now, we shall calculate the \\(Var(C(C,Q))\\) and find the conditions that minimize it. \\[\\begin{aligned} Var(cost(C,Q)) &amp;= Var\\left(\\frac{1}{m}\\sum\\limits_{c\\in C}\\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}Var\\left(\\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\mathbb{E}\\left(\\left(\\mu_C(c)f_Q(c)\\right)^2\\right) - \\left(\\mathbb{E}\\left(\\mu_C(c)f_Q(c)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\sum\\limits_{x\\in X}q(x)\\left(\\mu_C(x)f_Q(x)\\right)^2 - \\left(\\sum\\limits_{x\\in X}\\left(q(x)\\mu_C(x)f_Q(x)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\sum\\limits_{x\\in X}q(x)\\left(\\frac{1}{nq(x)}f_Q(x)\\right)^2 - \\left(\\sum\\limits_{x\\in X}\\left(q(x)\\frac{1}{nq(x)}f_Q(x)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{n^2m}\\left(\\sum\\limits_{x\\in X}\\frac{(f_Q(x))^2}{q(x)} - \\left(cost(X,Q)\\right)^2\\right) \\end{aligned}\\] If we choose \\(q(x) = \\frac{f_Q(x)}{\\sum\\limits_{x \\in X}f_Q(x)}\\), \\(Var(cost(C,Q)) = 0\\). Intuitively it is sensible, because we give more priority to the points that individually contribute more in the total cost (\\(f_Q(x)\\) is high). We have done this calculation for a single set of clusters \\(Q\\), but we need to generelize this to any \\(Q\\). Note that currently \\(q(x)\\) depends on \\(Q\\) which should be avoided for generality. To generalize importance sampling further, we need a concept called sensitivity, which is intuitively proportinal to \\(q(x)\\) but is more general and do not depend on any particular \\(Q\\). 3.3.1 Sensitivity Sensitivity \\(\\sigma(x)\\) is defined as the following for all \\(Q, Q \\in \\mathcal{Q}\\), where \\(\\mathcal{Q}\\) is the set of all possible cluster centers (really? how to find all possible cluster centers? we do not need to find them. We have other tricks to do that which is currently out of scope for this report. Please refer to Bachem, Lucic, and Krause (2017) later). \\[ \\sigma(x) = \\sup\\limits_{Q \\in \\mathcal{Q}}\\frac{f_Q(x)}{cost(X,Q)} \\] We can see that optimal \\(Q\\) might not be the same for all \\(x\\) and thus calculating \\(\\sigma(x)\\) might be another problem. But, we introduce a general upper bound \\(s(x)\\) on \\(\\sigma(x)\\). We have average sensitivity \\(S = \\frac{1}{n}\\sum\\limits_{x \\in X}s(x)\\). Now, we can modify our sampling distribution \\(q(x)\\) as following, \\[ q(x) = \\frac{1}{n}\\frac{s(x)}{S} = \\frac{s(x)}{\\sum\\limits_{x \\in X}s(x)} \\] Now, we try to get an estimate of lower bound on \\(m\\). Consider the following function \\(g_Q(x)\\). \\[ g_Q(x) = \\frac{f_Q(x)}{n\\cdot cost(X,Q)}\\frac{1}{Sq(x)} \\] Using Hoeffding’s inequality, we have the following formula, \\[ P\\left(\\left|\\mathbb{E}(g_Q(x)) - \\frac{1}{m}\\sum\\limits_{x \\in X}g_Q(x)\\right| &gt; \\varepsilon&#39;\\right) \\le 2\\exp\\left(-2m\\varepsilon&#39;^2\\right) \\] One can verify that \\(|\\mathbb{E}(g_Q(x)) = \\frac{1}{S}\\) and \\(\\frac{1}{m}\\sum\\limits_{x \\in X}g_Q(x) = \\frac{cost(C,Q)}{Scost(X,Q)}\\). Using the same result, we get, \\[ P(\\left|cost(X,Q) - cost(X,Q)\\right| &gt; \\varepsilon&#39;Scost(X,Q)) \\le 2\\exp\\left(-2m\\varepsilon&#39;^2\\right) \\] Hence, we can say that if \\(C\\) is \\(\\varepsilon\\)-coreset of \\(X\\) with atleast (\\(1-\\delta\\)) probability, estimate of \\(m\\) is the following, \\[ m \\ge \\frac{S^2}{2\\varepsilon^2}\\log_e\\frac{2}{\\delta} \\] We can see that, \\(m \\propto S^2\\), in case we consider \\(s(x)=n\\) and so effectively \\(S=n\\), we have \\(m \\propto n^2\\) (which is worst than original dataset itself). But, we have other ways to create tighter bounds \\(s(x)\\) to build useful coresets. Note that, tighter the bound \\(s(x)&gt;\\sigma(x)\\), better coresets we get. 3.3.2 Rough approximation Using the rough approximation techniques, we can find a theoretically bounded approximation of \\(cost(X, Q)\\) with much lesser computational power. One of the approach used here is known as \\((\\alpha, \\beta)\\) bi-criterion approximation. \\((\\alpha, \\beta)\\) approximation states that, for a set of cluster centers \\(Q_B\\) of cardinality \\(|\\beta K|\\), the following property holds, \\[ cost(X, Q_B) &lt; \\alpha\\;cost(X, Q^*_X) \\] Arthur and Vassilvitskii (2007) give an efficient algorithm that holds \\((\\alpha, \\beta)\\) bi-criterian. Traditionally, the algorithm is known as \\(D^2\\) sampling algorithm, which is also given below, Algorithm 1: \\(D^2\\) sampling Require: dataset \\(X\\), number of clusters \\(K\\). Sample \\(x\\) from \\(X\\) uniform randomly or preavailable weights. set \\(Q_B=\\{x\\}\\) for i \\(\\to\\) \\(2, 3, ..., K\\) do sample \\(x\\) from \\(X\\) with probability \\(p(x) = \\frac{d(x, Q_B)^2}{\\sum\\limits_{x&#39; \\in X}d(x&#39;, Q_B)^2}\\) and add \\(x\\) to \\(Q_B\\). return \\(Q_B\\) Arthur and Vassilvitskii (2007) also show that the following result holds with atleast \\(\\delta\\) probability, when \\(Q^*_B\\) is best cluster centers selected by running the \\(D^2\\) sampling algorithm \\(\\log_2\\frac{1}{1-\\delta}\\) times. \\[ cost(X, Q^*_B) \\le 16(\\log_2K+2)cost(X, Q^*_X) \\] 3.3.3 Bounding sensitivity Now, the final Lemma combines all the concepts we have seen thus far and gives a tighter bound on sensitivity \\(s(x)\\). For each point \\(x \\in X\\), we define a set of points \\(X_x\\) that share a common cluster center \\(b_x \\in Q^*_B\\), then the sensitivity \\(\\sigma(x)\\) is bounded by, \\[\\begin{aligned} \\bar{c}_B &amp;= \\frac{1}{n}\\sum\\limits_{x \\in X}d(x, b_x)\\\\ s(x) &amp;= \\frac{2 \\alpha\\;d(x, b_x)^2}{\\bar{c}_B} + \\frac{4\\alpha\\;\\sum\\limits_{x \\in X_x}d(x, b_x)}{|X_x|\\bar{c}_B} + \\frac{4n}{|X_x|}\\\\ S &amp;= 6\\alpha + 4K \\end{aligned}\\] This result also holds for any \\(Q \\in \\mathcal{Q}\\). Please refer to Bachem, Lucic, and Krause (2017) for the proof and subtle details. 3.3.4 Algorithm to create KMeans coreset Now, combining all the steps, algorithm to generate KMeans clustering can be given as the following, Algorithm 2: Coreset construction for KMeans clustering Require: dataset \\(X\\), number of clusters \\(K\\). Run \\(D^2\\) algorithm multiple times on the original dataset \\(X\\) to get \\(Q^*_B\\). calculate sensitivity scores \\(s(x)\\) and effectively \\(S\\) calculate probability distribution \\(q(x) = \\frac{s(x)}{nS}\\) sample a set of points \\(C\\) from \\(X\\) using \\(q(x)\\) til the coreset property is satisfied. Run Weighted KMeans algorithm on \\(C\\) considering the weights \\(\\mu_C(x) = \\frac{1}{q(x)}\\) (not \\(\\frac{1}{nq(x)}\\) because \\(n\\) will be anyway considered in the average cost) Resultant cluster centers set \\(Q^*_C\\) is an approximated set of cluster centers theoretically closer to \\(Q^*_X\\). In the next chapter, we will actually implement the importance sampling in coresets for KMeans clustering step by step. "],["coresets-for-kmeans-practical.html", "Coresets: 4 Coresets for KMeans (Practical) 4.1 Pseudo-data with 4 clusters", " Coresets: 4 Coresets for KMeans (Practical) In this practical, we will try to create an \\(\\varepsilon\\)-coreset (\\(\\varepsilon=0.3\\)) for KMeans clustering. Let us import some packages and set up a few useful functions, from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from scipy.spatial.distance import cdist, pdist import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) rc(&#39;font&#39;, size=16) rc(&#39;text&#39;, usetex=True) def plot_essentials(): # essential code for every plot hand, labs = plt.gca().get_legend_handles_labels() if len(hand)&gt;0: plt.legend(hand, labs); plt.tight_layout(); plt.show() plt.close() 4.1 Pseudo-data with 4 clusters Now, we generate a pseudo-dataset with 4 clusters. N = 100 K = 4 d = 2 epsilon = 0.3 c4X, c4y = make_blobs(n_samples=N, centers=K, n_features=d, random_state=0, cluster_std=0.8) plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We run the KMeans algorithm on full dataset to calculate \\(cost(X, Q^*_X)\\). Note that, in practice, this is infeasible because of superlinear time-complexity, but, we perform this step to have a comparison between \\(cost(X, Q^*_X)\\) and \\(cost(X, Q^*_C)\\). full_model = KMeans(n_clusters=K, random_state=0) full_model.fit(c4X); cost_QX = full_model.inertia_/N Optimal cost on the full dataset is \\(cost(X, Q^*_X)=\\) 1.16. Q_X = full_model.cluster_centers_ plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, label=&#39;$Q^*_X$&#39;, c=&#39;green&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plt.title(&#39;Optimal cluster centers on the original dataset&#39;); plot_essentials() Next step is running the \\(D^2\\) sampling algorithm on the full dataset to get the approximate centers \\(Q^*_B\\). To ensure that \\((\\alpha, \\beta)\\) criterion holds with probability \\(0.9\\), we need to run \\(D^2\\) sampling \\(\\log_2\\left(\\frac{1}{1-0.9}\\right) \\approx 4\\) times and select the best clustering. The best clusturing is defined as minimum value of \\(cost(X,Q^*_B)\\). cost_QB = np.inf for trial in range(4): np.random.seed(trial) fst_idx = np.random.choice(N) # Choosing first center randomly B = [] # Approximate cluster centers B.append(c4X[fst_idx]) for choice in range(K-1): # Choice of remaining K-1 centers proba = np.square(cdist(c4X, np.array(B))).min(axis=1) norm_proba = proba/np.sum(proba) idx = np.random.choice(N, p=norm_proba) B.append(c4X[idx,:]) tmp_cost = np.square(cdist(c4X, np.array(B)).min(axis=1)).sum()/N if tmp_cost&lt;cost_QB: cost_QB = tmp_cost B_star = B.copy() As per the \\((\\alpha, \\beta)\\) criterion, \\(cost(X,Q^*_B) \\le \\alpha\\cdot cost(X,Q^*_X)\\), where \\(\\alpha=16(\\log_2K+2)cost(X,Q^*_X)\\). Thus, ratio \\(R(Q^*_B, Q^*_X) = \\frac{cost(X,Q^*_B)}{cost(X,Q^*_X)} \\le \\alpha\\). Lower the ratio, better the approximation we have. Let us see how much ratio we get experimentally. alpha = 16*(np.log2(K) + 2) alpha_R = cost_QB/cost_QX; \\(\\alpha=\\) 64 and experimental ratio is 1.59. The experimental ratio is much lower than the upper limit, which is expected given the well-separated clusters and small data. Let us visualize the approximate centers \\(Q^*_B\\). Q_B = np.array(B_star) plt.scatter(c4X[:,0], c4X[:,1], c=c4y, alpha=0.5); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, c=&#39;green&#39;, label=&#39;$Q^*_X$&#39;); plt.scatter(Q_B[:,0], Q_B[:,1], s=200, marker=&#39;d&#39;, c=&#39;red&#39;, label=&#39;$Q^*_B$&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() Now, we will perform the importance sampling to select the coreset points. Before that, following are the essential variables need to be computed to calculate sensitivity and probability for the importance sampling. B_y = cdist(c4X, Q_B).argmin(axis=1); # Cluster labels according to Q_B cost_QB = np.square(cdist(c4X, Q_B)).min(axis=1); # Cost(X, Q_B) mean_cost_QB = np.mean(cost_QB); cost_QB_cluster = [cost_QB[B_y==C].mean() for C in range(K)]; # Cost of each cluster cluster_n = pd.Series(B_y).sort_index().value_counts().values # Cardinality of each cluster S = 6*alpha + 4*K; # Total sensitivity We can calculate the sensitivity and probability for each data-point as following, # Sensitivity s_x = np.array([2*alpha*cost_QB[i]/mean_cost_QB +\\ 4*alpha*cost_QB_cluster[B_y[i]]/mean_cost_QB +\\ 4*N/cluster_n[B_y[i]] for i in range(N)]) # Probability q_x = s_x/S/N assert np.sum(q_x).round(2) == 1. q_x = q_x/q_x.sum() # Adjusting for numerical precision to make sum(q)=1 Let us visualize the probabilities of the data-points. map_ax = plt.scatter(c4X[:,0], c4X[:,1], c=q_x); plt.colorbar(map_ax); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We have completed all the pre-requisite steps for importance sampling. Now, let us perform the importance sampling. Initially, we will set the coreset size to only 10% of the total data-points. np.random.seed(0) C = int(0.1*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, we can run the Weighted KMeans algorithm Abras and Balları́n (2005) to get unbiased cluster centers \\(Q^*_C\\). After that, we can visually see the minimum number of samples for which the coreset property holds (when \\(Q^*_C\\) becomes a good approximation for \\(Q^*_X\\) according to definition of coresets). cost_QC = [] # cost(X, Q_C) for n_points in range(K, C+K): tmp_model = KMeans(n_clusters=K, random_state=0); tmp_model = tmp_model.fit(c4C[:n_points], sample_weight=Cw[:n_points]); Q_C = tmp_model.cluster_centers_ tmp_cost = np.square(cdist(c4X, Q_C)).min(axis=1).sum()/N cost_QC.append(tmp_cost) plt.plot(range(K, C+K), np.array(cost_QC)/cost_QX, &#39;o-&#39;, markersize=5); plt.hlines(1, *plt.xlim(), label=&#39;$1$&#39;); plt.hlines(1+3*epsilon, *plt.xlim(), label=&#39;$1+3 \\\\varepsilon $&#39;, color=&#39;red&#39;); plt.xlabel(&#39;Number of coreset points&#39;);plt.ylabel(&#39;$\\\\frac{cost(X, Q_C)}{cost(X, Q_X)}$&#39;); plot_essentials() We see that coreset property holds at a substantially smaller number of points. However, number of coreset points do not depend on number of datapoints \\(N\\), but it depends on number of clusters \\(K\\), dimension of data \\(d\\), \\(\\varepsilon\\) and minimum probability \\(\\delta\\) for coreset. The upper bound defined by Bachem, Lucic, and Krause (2017) suggests drawing nearly 2000 points for our current settings, but we can see that, in practice, coreset property holds with much lesser number of coreset points. Theoretical bound is given as the following, \\[ \\text{Number of coreset points } m = \\Omega\\left( \\frac{dK^3\\log(K)+K^2\\log(\\frac{1}{\\delta})}{\\varepsilon^2}\\right) \\] We will draw a coreset with size 20% of the original dataset and fix it as the coreset for further analysis. np.random.seed(0) C = int(0.2*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, let us visualize the cluster centers \\(Q^*_C\\) found by running KMeans on the coreset and cluster centers \\(Q^*_X\\) found by running KMeans on the original dataset (again, just to demonstrate). full_model = KMeans(n_clusters=K, random_state=0) full_model = full_model.fit(c4X) cost_QX = -full_model.score(c4X) # cost(X, Q_X) (score is opposite of cost) Q_X = full_model.cluster_centers_ X_labels = full_model.predict(c4X) coreset_model = KMeans(n_clusters=K, random_state=0) coreset_model = coreset_model.fit(c4C, sample_weight=Cw) cost_QC = -coreset_model.score(c4X) # cost(X, Q_C) Q_C = coreset_model.cluster_centers_ C_labels = coreset_model.predict(c4X) colors1 = [&#39;tab:blue&#39;, &#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;m&#39;] mapper1 = lambda x: [colors1[i] for i in x] colors2 = [&#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;tab:blue&#39;, &#39;m&#39;] mapper2 = lambda x: [colors2[i] for i in x] plt.scatter(c4X[:,0], c4X[:,1], c=mapper1(X_labels), marker=&#39;&gt;&#39;, label=&#39;clusters of $Q^*_X$&#39;, s=50); plt.scatter(c4X[:,0], c4X[:,1], c=mapper2(C_labels), marker=&#39;&lt;&#39;, label=&#39;clusters of $Q^*_C$&#39;, s=50); plt.scatter(Q_X[:,0], Q_X[:,1], c=&#39;k&#39;, marker=&#39;&gt;&#39;, s=100, label=&#39;$Q^*_X$&#39;); plt.scatter(Q_C[:,0], Q_C[:,1], c=&#39;k&#39;, marker=&#39;&lt;&#39;, s=100, label=&#39;$Q^*_C$&#39;); plot_essentials() \\(cost(X, Q^*_X)=\\) 116 \\(cost(X, Q^*_C)=\\) 156 \\(R=\\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\\) 1.343 \\(&lt; \\frac{1+\\varepsilon}{1-\\varepsilon}=\\) 1.857 We can see that \\(Q^*_C\\) is able to approximate the \\(Q^*_X\\) efficiently as per our requirement of \\(\\varepsilon\\)-coreset. "],["references.html", "References", " References Abras, Guillermo N, and Virginia Laura Balları́n. 2005. “A Weighted k-Means Algorithm Applied to Brain Tissue Classification.” Journal of Computer Science &amp; Technology 5. Arthur, David, and Sergei Vassilvitskii. 2007. “K-Means++: The Advantages of Careful Seeding.” In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, 1027–35. SODA ’07. USA: Society for Industrial; Applied Mathematics. Bachem, Olivier, Mario Lucic, and Andreas Krause. 2017. “Practical Coreset Constructions for Machine Learning.” http://arxiv.org/abs/1703.06476. "]]
