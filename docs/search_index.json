[["index.html", "IN792: Coresets Coresets: 1 Preface", " IN792: Coresets Zeel B Patel 2021-05-08 Coresets: 1 Preface This report is generated for work done in the IN-792 project with Prof. Anirban Dasgupta in Semester-II of the academic year 2020-2021. Please go to GitHub discussions for any Q&amp;A, suggestions, or error reporting. "],["coresets.html", "Coresets: 2 Coresets 2.1 What are coresets?", " Coresets: 2 Coresets 2.1 What are coresets? An accurate definition of coresets is the following given by Bachem, Lucic, and Krause (2017), Coresets are small, weighted summaries of large data sets such that solutions found on the summary itself are provably competitive with solutions found on the entire data set. More intuitively, coresets are small subsets of the larger data (with weights associated with each data point) that can represent the original data with a good approximation on a particular model/algorithm in terms of cost. Coresets are extremely useful for models that do not scale well with data. If a model can not be optimized further on time complexity, using coresets is a realistic option to cope up with training time. Theoretical guarantee is a unique property about coresets over other approximation techniques, making it robust for practical usage. In the next chapter, we shall see the introduction to coresets for the KMeans clustering algorithm. "],["KM.html", "Coresets: 3 Coresets for KMeans (Theory) 3.1 Problem formulation 3.2 Uniform sampling 3.3 Importance sampling", " Coresets: 3 Coresets for KMeans (Theory) This chapter’s motive is to highlight the key ideas behind KMeans coreset construction and give intuitive proofs for some of the ideas. For more details, please refer to Bachem, Lucic, and Krause (2017). 3.1 Problem formulation In the KMeans clustering problem, we aim to cluster the dataset \\(X \\in R^d\\) of cardinality \\(n\\) in \\(K\\) seperate clusters. If \\(Q\\) is a set of cluster centers for KMeans in this problem, we can define the cost as the following, \\[ cost(X, Q) = \\frac{1}{n}\\sum\\limits_{x\\in X}f_Q(x) = \\frac{1}{n}\\sum\\limits_{x\\in X}\\min\\limits_{q\\in Q}||x-q||^2_2 \\] If \\(C\\) of cardinality \\(n&#39;\\) is a weighted coreset constructed from dataset \\(X\\), \\(n&#39;&lt;&lt;n\\) is our desired property. Note that \\(c \\in C\\) are i.i.d. samples. \\(C\\) is a valid \\(\\varepsilon\\)-coreset of \\(X\\) if the following property holds for any \\(Q\\) with high probability \\(\\delta\\) (\\(\\delta\\) can be quantified), \\[ |cost(X, Q) - cost(C,Q)| \\le \\varepsilon cost(X,Q) \\] There is a derived result of the above theorem, which is more useful in practice. If \\(Q^*_X\\) is optimal cluster centers obtained by executing KMeans on full dataset \\(X\\) and \\(Q^*_C\\) is optimal cluster centers obtained by executing KMeans on coreset \\(C\\), the following property holds with high probability, \\[ cost(X,Q^*_X) \\le cost(X, Q^*_C) \\le \\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X) \\] Note that, in practice, computing \\(Q^*_X\\) is not feasible. Thus we are willing to bear a higher cost up to \\(\\frac{1+\\varepsilon}{1-\\varepsilon}cost(X, Q^*_X)\\) at the benefit of reduced computational time. This property ensures that cost on the coreset stays within the defined upper bound without actually computing \\(Q^*_X\\) (which is obvious but just being explicit). We need to find a method to construct the coreset \\(C\\) such that the above properties hold with high probability (yes, nothing is deterministic in the probabilistic world). Let us try a naive method first. 3.2 Uniform sampling The first idea that comes to mind for coreset construction is uniform random sampling. We will mathematically see if a coreset constructed by uniform random sampling is useful or not. First we need to verify if \\(cost(C, Q)\\) is an unbiased estimator of \\(cost(X, Q)\\). Intuitively, we can say that, over multiple choices of coresets \\(C\\), we expect value of \\(cost(C, Q)\\) to stay closer to \\(cost(X, Q)\\) and if we take \\(\\mathbb{E}_C(cost(C,Q))\\) over a long run, it should converge to \\(cost(X, Q)\\). Now, let us prove the same as following, \\[\\begin{aligned} \\mathbb{E}_C(cost(C, Q)) &amp;= \\mathbb{E}_C\\left(\\frac{1}{m}\\sum\\limits_{c \\in C}f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\mathbb{E}(f_Q(c))\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}\\frac{1}{n}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}cost(X,Q)\\\\ &amp;= cost(X, Q) \\end{aligned}\\] We saw that \\(cost(C,Q)\\) converges to \\(cost(X,Q)\\) in expectation. But, we need to minimize \\(Var(cost(C,Q))\\) as well, so that \\(cost(C,Q)\\) is as close as possible to \\(cost(X,Q)\\). \\[\\begin{aligned} Var(cost(C,Q)) &amp;= Var(\\frac{1}{m}\\sum\\limits_{c \\in C} f_Q(c))\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c \\in C}Var(f_Q(c))\\\\ &amp;\\le \\frac{1}{m^2}\\sum\\limits_{c \\in C}\\mathbb{E}(f_Q(c)^2) = \\frac{1}{m^2}\\sum\\limits_{c \\in C}\\frac{1}{n}\\sum\\limits_{x \\in X}f_Q(x)^2\\\\ &amp;\\le \\frac{1}{nm^2}\\sum\\limits_{c \\in C}\\left(\\sum\\limits_{x \\in X}f_Q(x)\\right)^2\\\\ &amp;= \\frac{1}{nm^2}\\sum\\limits_{c \\in C}\\left(n\\cdot cost(X,Q)\\right)^2\\\\ &amp;= \\frac{n}{m^2}\\sum\\limits_{c \\in C}cost(X,Q)^2\\\\ &amp;= \\frac{n}{m}cost(X,Q)^2 \\end{aligned}\\] We can see that \\(m \\to \\infty\\) then \\(Var(cost(C,Q)) \\to 0\\). Let us get an estimate of \\(m\\) using Chebyshev’s inequality. \\[ P\\left(|cost(C, Q) - cost(X, Q)| \\ge k\\sqrt{\\frac{n}{m}}cost(X,Q)\\right) \\le \\frac{1}{k^2} \\] Substituting \\(k\\sqrt{\\frac{n}{m}} = \\varepsilon \\to k = \\varepsilon\\sqrt{\\frac{m}{n}}\\). \\[ \\delta = P\\left(|cost(C, Q) - cost(X, Q)| \\ge \\varepsilon \\cdot cost(X,Q)\\right) \\le \\frac{n}{\\varepsilon^2m}\\\\ \\] The above expression suggests that if we are interested in \\(\\varepsilon\\)-coreset with atleast (\\(1-\\delta\\)) probability, we need to discard all \\(m \\le \\frac{n}{\\varepsilon^2\\delta}\\). Thus we have \\(m \\ge \\frac{n}{\\varepsilon^2\\delta}\\). The same phenomenon is illustrated in the below plot, ## Text(0.5, 1.0, &#39;$n = 100$, $\\\\varepsilon=0.3$&#39;) Looking at the numbers, to construct the coreset with atleast 70% probability for dataset of size \\(n=100\\), we need to sample atleast \\(m=\\frac{n}{\\varepsilon^2\\delta}=4000\\) points. This is clearly not useful. Also, note that \\(m \\propto n\\), so, coreset size will increase as \\(n\\) increases maintaining \\(m&gt;&gt;n\\). Thus, this method fails to construct a coreset. One may want to have a look at an intuitive example given in Bachem, Lucic, and Krause (2017). We need to change our sampling strategy to build better coresets. Let us move to importance sampling now. 3.3 Importance sampling Let us say we sample each point \\(x\\) with probability \\(q(x)\\). In this process, we might not have \\(cost(C,Q)\\) as an unbiased estimator of \\(cost(X,Q)\\) anymore. Thus, we need to introduce weights \\(\\mu_C(c), c \\in C\\). We will use the weighted \\(cost(C, Q)\\) as defined below, \\[ cost(C, Q) = \\frac{1}{m}\\sum\\limits_{c \\in C} \\mu_C(c)f_Q(c) \\] Let us find out a value of \\(\\mu_C(c)\\) such that \\(\\mathbb{E}_C(cost(C,Q))\\) converges to \\(cost(X, Q)\\) \\[\\begin{aligned} \\mathbb{E}_C(cost(C,Q)) &amp;= \\mathbb{E}_C\\left(\\frac{1}{m}\\sum\\limits_{c \\in C} \\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\mathbb{E}(\\mu_C(c)f_Q(c))\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}q(x)\\mu_C(x)f_Q(x)\\\\ \\text{Now, we should take }\\mu_C(x) = \\frac{1}{nq(x)}\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\sum\\limits_{x \\in X}q(x)\\frac{1}{nq(x)}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}\\frac{1}{n}\\sum\\limits_{x \\in X}f_Q(x)\\\\ &amp;= \\frac{1}{m}\\sum\\limits_{c \\in C}cost(X, Q)\\\\ &amp;= cost(X, Q) \\end{aligned}\\] Now, we shall calculate the \\(Var(C(C, Q))\\) and find the conditions that minimize it. \\[\\begin{aligned} Var(cost(C,Q)) &amp;= Var\\left(\\frac{1}{m}\\sum\\limits_{c\\in C}\\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}Var\\left(\\mu_C(c)f_Q(c)\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\mathbb{E}\\left(\\left(\\mu_C(c)f_Q(c)\\right)^2\\right) - \\left(\\mathbb{E}\\left(\\mu_C(c)f_Q(c)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\sum\\limits_{x\\in X}q(x)\\left(\\mu_C(x)f_Q(x)\\right)^2 - \\left(\\sum\\limits_{x\\in X}\\left(q(x)\\mu_C(x)f_Q(x)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{m^2}\\sum\\limits_{c\\in C}\\left(\\sum\\limits_{x\\in X}q(x)\\left(\\frac{1}{nq(x)}f_Q(x)\\right)^2 - \\left(\\sum\\limits_{x\\in X}\\left(q(x)\\frac{1}{nq(x)}f_Q(x)\\right)\\right)^2\\right)\\\\ &amp;= \\frac{1}{n^2m}\\left(\\sum\\limits_{x\\in X}\\frac{(f_Q(x))^2}{q(x)} - \\left(cost(X,Q)\\right)^2\\right) \\end{aligned}\\] If we choose \\(q(x) = \\frac{f_Q(x)}{\\sum\\limits_{x \\in X}f_Q(x)}\\), \\(Var(cost(C,Q)) = 0\\). Intuitively it is sensible, because we give more priority to the points that individually contribute more in the total cost (\\(f_Q(x)\\) is high). We have done this calculation for a single set of clusters \\(Q\\), but we need to generalize this to any \\(Q\\). Note that currently, \\(q(x)\\) depends on \\(Q\\), which should be avoided for generality. To generalize importance sampling further, we need a concept called sensitivity, which is intuitively proportional to \\(q(x)\\) but is more general and does not depend on any particular \\(Q\\). 3.3.1 Sensitivity Sensitivity \\(\\sigma(x)\\) is defined as the following for all \\(Q, Q \\in \\mathcal{Q}\\), where \\(\\mathcal{Q}\\) is the set of all possible cluster centers (really? How do we find all possible cluster centers? We do not need to find them. We have other tricks to do that which is currently out of scope for this report. Please refer to Bachem, Lucic, and Krause (2017) later). \\[ \\sigma(x) = \\sup\\limits_{Q \\in \\mathcal{Q}}\\frac{f_Q(x)}{cost(X,Q)} \\] We can see that optimal \\(Q\\) might not be the same for all \\(x\\) and thus calculating \\(\\sigma(x)\\) might be another problem. But, we introduce a general upper bound \\(s(x)\\) on \\(\\sigma(x)\\). We have average sensitivity \\(S = \\frac{1}{n}\\sum\\limits_{x \\in X}s(x)\\). Now, we can modify our sampling distribution \\(q(x)\\) as following, \\[ q(x) = \\frac{1}{n}\\frac{s(x)}{S} = \\frac{s(x)}{\\sum\\limits_{x \\in X}s(x)} \\] Now, we try to get an estimate of a lower bound on \\(m\\). Consider the following function \\(g_Q(x)\\). \\[ g_Q(x) = \\frac{f_Q(x)}{n\\cdot cost(X,Q)}\\frac{1}{Sq(x)} \\] Using Hoeffding’s inequality, we have the following formula, \\[ P\\left(\\left|\\mathbb{E}(g_Q(x)) - \\frac{1}{m}\\sum\\limits_{x \\in X}g_Q(x)\\right| &gt; \\varepsilon&#39;\\right) \\le 2\\exp\\left(-2m\\varepsilon&#39;^2\\right) \\] One can verify that \\(|\\mathbb{E}(g_Q(x)) = \\frac{1}{S}\\) and \\(\\frac{1}{m}\\sum\\limits_{x \\in X}g_Q(x) = \\frac{cost(C,Q)}{Scost(X,Q)}\\). Using the same result, we get, \\[ P(\\left|cost(X,Q) - cost(X,Q)\\right| &gt; \\varepsilon&#39;Scost(X,Q)) \\le 2\\exp\\left(-2m\\varepsilon&#39;^2\\right) \\] Hence, we can say that if \\(C\\) is \\(\\varepsilon\\)-coreset of \\(X\\) with atleast (\\(1-\\delta\\)) probability, estimate of \\(m\\) is the following, \\[ m \\ge \\frac{S^2}{2\\varepsilon^2}\\log_e\\frac{2}{\\delta} \\] We can see that, \\(m \\propto S^2\\), in case we consider \\(s(x)=n\\) and so effectively \\(S=n\\), we have \\(m \\propto n^2\\) (which is worst than original dataset itself). But, we have other ways to create tighter bounds \\(s(x)\\) to build useful coresets. Note that, tighter the bound \\(s(x)&gt;\\sigma(x)\\), better coresets we get. 3.3.2 Rough approximation Using the rough approximation techniques, we can find a theoretically bounded approximation of \\(cost(X, Q)\\) with much lesser computational power. One of the approaches used here is \\((\\alpha, \\beta)\\) bi-criterion approximation. \\((\\alpha, \\beta)\\) approximation states that, for a set of cluster centers \\(Q_B\\) of cardinality \\(|\\beta K|\\), the following property holds, \\[ cost(X, Q_B) &lt; \\alpha\\;cost(X, Q^*_X) \\] Arthur and Vassilvitskii (2007) propose an efficient algorithm that holds \\((\\alpha, \\beta)\\) bi-criterion. Traditionally, the algorithm is known as the \\(D^2\\) sampling algorithm, which is given in Section 6, Arthur and Vassilvitskii (2007) also shows that the following result holds with at least \\(\\delta\\) probability when \\(Q^*_B\\) is best cluster centers selected by running the \\(D^2\\) sampling algorithm \\(\\log_2\\frac{1}{1-\\delta}\\) times. \\[ cost(X, Q^*_B) \\le 16(\\log_2K+2)cost(X, Q^*_X) \\] 3.3.3 Bounding sensitivity Now, the final Lemma combines all the concepts we have seen thus far and gives a tighter bound on sensitivity \\(s(x)\\). For each point \\(x \\in X\\), we define a set of points \\(X_x\\) that share a common cluster center \\(b_x \\in Q^*_B\\), then the sensitivity \\(\\sigma(x)\\) is bounded by, \\[\\begin{aligned} \\bar{c}_B &amp;= \\frac{1}{n}\\sum\\limits_{x \\in X}d(x, b_x)\\\\ s(x) &amp;= \\frac{2 \\alpha\\;d(x, b_x)^2}{\\bar{c}_B} + \\frac{4\\alpha\\;\\sum\\limits_{x \\in X_x}d(x, b_x)}{|X_x|\\bar{c}_B} + \\frac{4n}{|X_x|}\\\\ S &amp;= 6\\alpha + 4K \\end{aligned}\\] This result also holds for any \\(Q \\in \\mathcal{Q}\\). Please refer to Bachem, Lucic, and Krause (2017) for the proof and subtle details. 3.3.4 Algorithm to create KMeans coreset Now, combining all the steps, the algorithm to generate KMeans clustering can be given as the following, Algorithm 2: Coreset construction for KMeans clustering Require: dataset \\(X\\), number of clusters \\(K\\). Run \\(D^2\\) algorithm multiple times on the original dataset \\(X\\) to get \\(Q^*_B\\). calculate sensitivity scores \\(s(x)\\) and effectively \\(S\\) calculate probability distribution \\(q(x) = \\frac{s(x)}{nS}\\) sample a set of points \\(C\\) from \\(X\\) using \\(q(x)\\) until the coreset property is satisfied. Run Weighted KMeans algorithm on \\(C\\) considering the weights \\(\\mu_C(x) = \\frac{1}{q(x)}\\) (not \\(\\frac{1}{nq(x)}\\) because \\(n\\) will be anyway considered in the average cost) Resultant cluster centers set \\(Q^*_C\\) is an approximated set of cluster centers theoretically closer to \\(Q^*_X\\). In the next chapter, we will implement the importance sampling in coresets for KMeans clustering step by step. "],["KMP.html", "Coresets: 4 Coresets for KMeans (Practical) 4.1 Pseudo-data with 4 clusters 4.2 Pseudo-dataset with 100 clusters and 10000 data-points", " Coresets: 4 Coresets for KMeans (Practical) In this practical, we will try to create an \\(\\varepsilon\\)-coreset (\\(\\varepsilon=0.3\\)) for KMeans clustering. Let us import some packages and set up a few useful functions, from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from scipy.spatial.distance import cdist, pdist import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) rc(&#39;font&#39;, size=16) rc(&#39;text&#39;, usetex=True) def plot_essentials(): # essential code for every plot hand, labs = plt.gca().get_legend_handles_labels() if len(hand)&gt;0: plt.legend(hand, labs); plt.tight_layout(); plt.show() plt.close() 4.1 Pseudo-data with 4 clusters Now, we generate a pseudo-dataset with 4 clusters. N = 100 K = 4 d = 2 epsilon = 0.3 c4X, c4y = make_blobs(n_samples=N, centers=K, n_features=d, random_state=0, cluster_std=0.8) plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We run the KMeans algorithm on the full dataset to calculate \\(cost(X, Q^*_X)\\). Note that, in practice, this is infeasible because of superlinear time-complexity, but we perform this step to have a comparison between \\(cost(X, Q^*_X)\\) and \\(cost(X, Q^*_C)\\). full_model = KMeans(n_clusters=K, random_state=0) full_model.fit(c4X); cost_QX = full_model.inertia_/N Optimal cost on the full dataset is \\(cost(X, Q^*_X)=\\) 1.16. Q_X = full_model.cluster_centers_ plt.scatter(c4X[:,0], c4X[:,1], c=c4y); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, label=&#39;$Q^*_X$&#39;, c=&#39;green&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plt.title(&#39;Optimal cluster centers on the original dataset&#39;); plot_essentials() The next step is running the \\(D^2\\) sampling algorithm on the full dataset to get the approximate centers \\(Q^*_B\\). To ensure that \\((\\alpha, \\beta)\\) criterion holds with probability \\(0.9\\), we need to run \\(D^2\\) sampling \\(\\log_2\\left(\\frac{1}{1-0.9}\\right) \\approx 4\\) times and select the best clustering (least cost). cost_QB = np.inf for trial in range(4): np.random.seed(trial) fst_idx = np.random.choice(N) # Choosing first center randomly B = [] # Approximate cluster centers B.append(c4X[fst_idx]) for choice in range(K-1): # Choice of remaining K-1 centers proba = np.square(cdist(c4X, np.array(B))).min(axis=1) norm_proba = proba/np.sum(proba) idx = np.random.choice(N, p=norm_proba) B.append(c4X[idx,:]) tmp_cost = np.square(cdist(c4X, np.array(B)).min(axis=1)).sum()/N if tmp_cost&lt;cost_QB: cost_QB = tmp_cost B_star = B.copy() As per the \\((\\alpha, \\beta)\\) criterion, \\(cost(X,Q^*_B) \\le \\alpha\\cdot cost(X,Q^*_X)\\), where \\(\\alpha=16(\\log_2K+2)cost(X,Q^*_X)\\). Thus, ratio \\(R(Q^*_B, Q^*_X) = \\frac{cost(X,Q^*_B)}{cost(X,Q^*_X)} \\le \\alpha\\). Lower the ratio, better the approximation we have. Let us see how much ratio we get experimentally. alpha = 16*(np.log2(K) + 2) alpha_R = cost_QB/cost_QX; \\(\\alpha=\\) 64 and experimental ratio is 1.59. The experimental ratio is much lower than the upper limit, which is expected given the well-separated clusters and small data. Let us visualize the approximate centers \\(Q^*_B\\). Q_B = np.array(B_star) plt.scatter(c4X[:,0], c4X[:,1], c=c4y, alpha=0.5); plt.scatter(Q_X[:,0], Q_X[:,1], s=200, marker=&#39;*&#39;, c=&#39;green&#39;, label=&#39;$Q^*_X$&#39;); plt.scatter(Q_B[:,0], Q_B[:,1], s=200, marker=&#39;d&#39;, c=&#39;red&#39;, label=&#39;$Q^*_B$&#39;); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() Now, we will perform the importance sampling to select the coreset points. The following are the essential variables that we need to compute to calculate sensitivity and probability for the importance sampling. B_y = cdist(c4X, Q_B).argmin(axis=1); # Cluster labels according to Q_B cost_QB = np.square(cdist(c4X, Q_B)).min(axis=1); # Cost(X, Q_B) mean_cost_QB = np.mean(cost_QB); cost_QB_cluster = [cost_QB[B_y==C].mean() for C in range(K)]; # Cost of each cluster cluster_n = pd.Series(B_y).sort_index().value_counts().values # Cardinality of each cluster S = 6*alpha + 4*K; # Total sensitivity We can calculate the sensitivity and probability for each data-point as following, # Sensitivity s_x = np.array([2*alpha*cost_QB[i]/mean_cost_QB +\\ 4*alpha*cost_QB_cluster[B_y[i]]/mean_cost_QB +\\ 4*N/cluster_n[B_y[i]] for i in range(N)]) # Probability q_x = s_x/S/N assert np.sum(q_x).round(2) == 1. q_x = q_x/q_x.sum() # Adjusting for numerical precision to make sum(q)=1 Let us visualize the probabilities of the data-points. map_ax = plt.scatter(c4X[:,0], c4X[:,1], c=q_x); plt.colorbar(map_ax); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plot_essentials() We have completed all the pre-requisite steps for importance sampling. Now, let us perform the importance sampling. Initially, we will set the coreset size to only 10% of the total data-points. np.random.seed(0) C = int(0.1*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, we can run the Weighted KMeans algorithm Abras and Balları́n (2005) to get unbiased cluster centers \\(Q^*_C\\). After that, we can visually see the minimum number of samples for which the coreset property holds (when \\(Q^*_C\\) becomes a good approximation for \\(Q^*_X\\) according to the definition of coresets). cost_QC = [] # cost(X, Q_C) cost_QC_biased = [] for n_points in range(K, C+K): # Unbiased estimate tmp_model = KMeans(n_clusters=K, random_state=0); tmp_model = tmp_model.fit(c4C[:n_points], sample_weight=Cw[:n_points]); Q_C = tmp_model.cluster_centers_ tmp_cost = np.square(cdist(c4X, Q_C)).min(axis=1).sum()/N cost_QC.append(tmp_cost) # Biased estimate tmp_model_biased = KMeans(n_clusters=K, random_state=0); tmp_model_biased = tmp_model_biased.fit(c4C[:n_points]); Q_C_biased = tmp_model_biased.cluster_centers_ tmp_cost_biased = np.square(cdist(c4X, Q_C_biased)).min(axis=1).sum()/N cost_QC_biased.append(tmp_cost_biased) plt.plot(range(K, C+K), np.array(cost_QC)/cost_QX, &#39;o-&#39;, markersize=5, label=&#39;unbiased ratio&#39;); plt.plot(range(K, C+K), np.array(cost_QC_biased)/cost_QX, &#39;o-&#39;, markersize=5, label=&#39;biased ratio&#39;); plt.hlines(1, *plt.xlim(), label=&#39;$1$&#39;); plt.hlines(1+3*epsilon, *plt.xlim(), label=&#39;$1+3 \\\\varepsilon $&#39;, color=&#39;red&#39;); plt.xlabel(&#39;Number of coreset points&#39;);plt.ylabel(&#39;$\\\\frac{cost(X, Q_C)}{cost(X, Q_X)}$&#39;); plot_essentials() We see that the coreset property holds at a substantially smaller number of points. However, the number of coreset points does not depend on the number of data-points \\(N\\), but it depends on the number of clusters \\(K\\), dimension of data \\(d\\), \\(\\varepsilon\\) and minimum probability \\(\\delta\\) for coreset. The upper bound defined by Bachem, Lucic, and Krause (2017) suggests drawing nearly 2000 points for our current settings, but we can see that, in practice, coreset property holds with a much lesser number of coreset points. Theoretical bound is given as the following, \\[ \\text{Number of coreset points } m = \\Omega\\left( \\frac{dK^3\\log(K)+K^2\\log(\\frac{1}{\\delta})}{\\varepsilon^2}\\right) \\] We will draw a coreset with a 20% size of the original dataset and fix it as the coreset for further analysis. np.random.seed(0) C = int(0.2*N) # Number of points to draw (coreset size) C_idx = np.random.choice(N, size=C+K, p=q_x) # Coreset index c4C = c4X[C_idx] # Coreset points Cw = 1/q_x[C_idx] # Corresponding weights Now, let us visualize the cluster centers \\(Q^*_C\\) found by running KMeans on the coreset and cluster centers \\(Q^*_X\\) found by running KMeans on the original dataset (again, just to demonstrate). full_model = KMeans(n_clusters=K, random_state=0) full_model = full_model.fit(c4X) cost_QX = -full_model.score(c4X) # cost(X, Q_X) (score is opposite of cost) Q_X = full_model.cluster_centers_ X_labels = full_model.predict(c4X) coreset_model = KMeans(n_clusters=K, random_state=0) coreset_model = coreset_model.fit(c4C, sample_weight=Cw) cost_QC = -coreset_model.score(c4X) # cost(X, Q_C) Q_C = coreset_model.cluster_centers_ C_labels = coreset_model.predict(c4X) colors1 = [&#39;tab:blue&#39;, &#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;m&#39;] mapper1 = lambda x: [colors1[i] for i in x] colors2 = [&#39;tab:orange&#39;, &#39;tab:brown&#39;, &#39;tab:blue&#39;, &#39;m&#39;] mapper2 = lambda x: [colors2[i] for i in x] plt.scatter(c4X[:,0], c4X[:,1], c=mapper1(X_labels), marker=&#39;&gt;&#39;, label=&#39;clusters of $Q^*_X$&#39;, s=50); plt.scatter(c4X[:,0], c4X[:,1], c=mapper2(C_labels), marker=&#39;&lt;&#39;, label=&#39;clusters of $Q^*_C$&#39;, s=50); plt.scatter(Q_X[:,0], Q_X[:,1], c=&#39;k&#39;, marker=&#39;&gt;&#39;, s=100, label=&#39;$Q^*_X$&#39;); plt.scatter(Q_C[:,0], Q_C[:,1], c=&#39;k&#39;, marker=&#39;&lt;&#39;, s=100, label=&#39;$Q^*_C$&#39;); plot_essentials() \\(cost(X, Q^*_X)=\\) 116 \\(cost(X, Q^*_C)=\\) 156 \\(R=\\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\\) 1.343 \\(&lt; \\frac{1+\\varepsilon}{1-\\varepsilon}=\\) 1.857 We can see that \\(Q^*_C\\) can approximate the \\(Q^*_X\\) efficiently as per our requirement of \\(\\varepsilon\\)-coreset. 4.2 Pseudo-dataset with 100 clusters and 10000 data-points Let us try the coreset construction on a more extensive dataset. First, we will generate a dataset with 100 clusters and 10000 points. I have hidden the code for better visuals and to avoid redundancy. Now, let us check the actual fit with vanilla KMeans algorithm, Now, we find approximate centers \\(Q^*_B\\) with \\(D^2\\) sampling, \\(\\alpha=\\) 138.3 and experimental ratio is 1.8. Let us visualize the probability distribution over all data points. Now, we visualize comparison between \\(cost(C,Q)\\) and \\(cost(X,Q)\\) as we increase \\(m\\). We see that \\(cost(C, Q)\\) starts satisfying coreset property well below 10% of the total dataset. Let us fix the coreset size at 10% of the entire dataset and check the effect on the coreset property. \\(cost(X, Q^*_X)=\\) 1585 \\(cost(X, Q^*_C)=\\) 1876 \\(R=\\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\\) 1.183 \\(&lt; \\frac{1+\\varepsilon}{1-\\varepsilon}=\\) 1.857 We can see that \\(Q^*_C\\) can approximate the \\(Q^*_X\\) efficiently as per our requirement of \\(\\varepsilon\\)-coreset. "],["coresets-for-linear-regression.html", "Coresets: 5 Coresets for Linear Regression 5.1 Coresets for linear regression (order=1) 5.2 Coresets for linear regression (order=5) 5.3 Checking uncertainty in the fit 5.4 Questions", " Coresets: 5 Coresets for Linear Regression We will see one of the methods to select coreset points for linear regression. We calculate “Ridge leverage scores” to create a sampling distribution for coreset selection. The process to calculate sampling distribution \\(p(X)\\) is as the following. \\(X_* = \\begin{bmatrix}X \\\\ \\lambda I_d\\end{bmatrix}\\) \\(U \\Sigma V^T = X_*\\) \\(\\mathbf{p}(X) = ||U(0:n, :)||_2^2\\) Now, let us implement this in a python function. from sklearn.linear_model import Ridge import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) rc(&#39;font&#39;, size=16) rc(&#39;text&#39;, usetex=True) def plot_essentials(): # essential code for every plot hand, labs = plt.gca().get_legend_handles_labels() if len(hand)&gt;0: plt.legend(hand, labs); plt.tight_layout(); plt.show() plt.close() def get_proba(x, lmd): x_list = [np.ones(x.shape[0]), x.ravel()] x_extra = np.vstack(x_list).T A = np.vstack([x_extra, np.eye(x_extra.shape[1])*np.sqrt(lmd)]) U, S, V = np.linalg.svd(A, full_matrices=False) U1 = U[:x.shape[0], :] proba = np.square(U1).sum(axis=1)/np.square(U1).sum(axis=1).sum() return proba We will sample the coresets for Scikit-learn diabetes dataset. 5.1 Coresets for linear regression (order=1) from sklearn.datasets import load_diabetes X, y = load_diabetes(return_X_y=True) X = X[:, np.newaxis, 2] X_train, y_train = X[:-20], y[:-20] X_test, y_test = X[-20:], y[-20:] plt.scatter(X_train, y_train, label=&#39;Train&#39;); plt.scatter(X_test, y_test, label=&#39;Test&#39;); plt.xlabel(&#39;X&#39;); plt.ylabel(&#39;Y&#39;); plot_essentials(); Now, let us visualize sampling probabilities of the train dataset. lmd = 0.0001 X_proba = get_proba(X_train, lmd); plt.scatter(X_train, y_train, c=X_proba, label=&#39;&#39;); plt.colorbar(); plt.title(&#39;Sampling probability distribution over training data&#39;); plt.xlabel(&#39;X&#39;); plt.ylabel(&#39;Y&#39;); plot_essentials(); Sampling 10% of the training data as a coreset. np.random.seed(0) N_core = len(X_train)//10 core_idx = np.random.choice(len(X_train), size=N_core, replace=True, p=X_proba) X_core = X_train[core_idx] y_core = y_train[core_idx] Checking fit on original dataset and on coreset. FullModel = Ridge(alpha=lmd).fit(X_train, y_train) CoreModel = Ridge(alpha=lmd).fit(X_core, y_core) plt.scatter(X_train, y_train, label=&#39;Train data&#39;); plt.scatter(X_core, y_core, label=&#39;Coreset&#39;); y_pred_full = FullModel.predict(X_train); y_pred_core = CoreModel.predict(X_train); plt.plot(X_train, y_pred_full, label=&#39;Fit on train&#39;); plt.plot(X_train, y_pred_core, label=&#39;Fit on coreset&#39;); plt.xlabel(&#39;X&#39;); plt.ylabel(&#39;Y&#39;); plot_essentials(); Let us do cost comparison to verify if this is an \\(\\varepsilon\\)-coreset. We take \\(\\varepsilon=0.3\\). eps = 0.3; FullModelCost = np.square(FullModel.predict(X_train) - y_train).sum()/len(X_train); CoreModelCost = np.square(CoreModel.predict(X_train) - y_train).sum()/len(X_train); \\[ \\frac{cost(X,Q^*_C)}{cost(X,Q^*_X)} \\le \\frac{1+\\varepsilon}{1-\\varepsilon} \\] We have above relationship 1.02 \\(\\le\\) 1.86, thus we found a valid \\(\\varepsilon\\)-coreset here. 5.2 Coresets for linear regression (order=5) Let us generate random data using order 5 polynomial kernel. import GPy np.random.seed(123) kernel = GPy.kern.Poly(1, order=5, bias=1, scale=1, variance=1) sigma_n = 1.1 N = 100 X = np.random.normal(0,1,N).reshape(-1,1) N = X.shape[0] cov_matrix = kernel.K(X, X) cov_matrix += np.eye(N)*sigma_n**2 y_poly = np.random.multivariate_normal(np.zeros(N), cov_matrix).reshape(-1,1) plt.scatter(X, y_poly,s=5); plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;y&#39;); plt.ylim(-10,30); plot_essentials(); Sampling uniform samples and coreset samples. def get_proba(x, lmd, order=1): x_list = [np.ones(x.shape[0])] for i in range(1,order+1): x_list.append(x.ravel()**i) x_extra = np.vstack(x_list).T A = np.vstack([x_extra, np.eye(x_extra.shape[1])*np.sqrt(lmd)]) U, S, V = np.linalg.svd(A, full_matrices=False) U1 = U[:x.shape[0], :] proba = np.square(U1).sum(axis=1)/np.square(U1).sum(axis=1).sum() return proba def get_coreset(x, y, n, lmd, order, seed): np.random.seed(seed) proba = get_proba(x, lmd, order) idx = np.random.choice(x.shape[0], size=n, p=proba) return idx def get_uniform(x, y, n, seed): np.random.seed(seed) proba = np.ones(x.shape[0])/x.shape[0] idx = np.random.choice(x.shape[0], size=n, p=proba) return idx lmd = (sigma_n/(kernel.variance)**0.5)**2 n_sample = len(y_poly)//10 order = 5 core_idx = get_coreset(X, y_poly, n_sample, lmd, order, seed=0) X_core, y_core = X[core_idx], y_poly[core_idx] uni_idx = get_uniform(X, y_poly, n_sample, seed=0) X_uni, y_uni = X[uni_idx], y_poly[uni_idx] Checking the fit using the same kernel (without optimizing the hyperparamaters because we have kept them constants for analysis). Fitting on the uniform samples. uniGP = GPy.models.GPRegression(X_uni, y_uni, kernel) uniGP.likelihood.variance = sigma_n**2 fig, ax = plt.subplots(); ax.scatter(X, y_poly, c=&#39;r&#39;, label=&#39;full data&#39;, alpha=0.5); uniGP.plot(ax=ax); ax.set_ylim(-10,30); ax.set_title(&#39;Fit on uniform samples&#39;); plot_essentials(); Fitting on the coreset samples. coreGP = GPy.models.GPRegression(X_core, y_core, kernel) coreGP.likelihood.variance = sigma_n**2 fig, ax = plt.subplots(); ax.scatter(X, y_poly, c=&#39;r&#39;, label=&#39;full data&#39;, alpha=0.5); coreGP.plot(ax=ax); ax.set_ylim(-10,30); ax.set_title(&#39;Fit on coreset samples&#39;); plot_essentials(); 5.3 Checking uncertainty in the fit np.random.seed(0); # X_new = X.copy() X_new = np.sort(np.random.normal(2,2,N*3)).reshape(-1,1); s = 20; # Full data train_X, train_y = X.copy(), y_poly.copy() tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;k&#39;, label=&#39;full data&#39;); # Uniform samples train_X, train_y = X[uni_idx], y_poly[uni_idx] tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;r&#39;, label=&#39;uniform&#39;); # Coreset samples train_X, train_y = X[core_idx], y_poly[core_idx] tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;g&#39;, label=&#39;coreset&#39;); plt.ylim(0,7); plt.xlabel(&quot;X&quot;);plt.ylabel(&quot;Uncertainty&quot;); plot_essentials(); We can see that uncertainty for the model fitted on coreset is closely matching with the model fitted on full dataset. Uncertainty is high for uniform sampling in the end regions because we have low density of input \\(X\\) in that region and thus points in those region are less likely to be included in the uniform samples. ## Comparing uncertainty with best coreset and uncertainty sampling (active learning) Sampling from an active learning technique called uncertainty sampling. all_idx = list(range(N)) train_idx = [0] pool_idx = all_idx.copy() for i in train_idx: pool_idx.remove(i) sidx = np.argsort(X.ravel()) def update(): # Computation train_X, train_y = X[train_idx], y_poly[train_idx] tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X[pool_idx]) meanall, varall = tmpGP.predict(X) std2all = np.sqrt(varall)*2 next_idx = pool_idx[np.argmax(var)] # Updating train_idx.append(next_idx) pool_idx.remove(next_idx) for i in range(15): update() np.random.seed(0); X_new = np.sort(np.random.normal(2,2,N*3)).reshape(-1,1); s = 20 # Full data train_X, train_y = X.copy(), y_poly.copy() tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;k&#39;, label=&#39;full data&#39;); # Uniform samples train_X, train_y = X[uni_idx], y_poly[uni_idx] tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;r&#39;, label=&#39;uniform&#39;); # Coreset samples train_X, train_y = X[core_idx], y_poly[core_idx] tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;g&#39;, label=&#39;coreset&#39;); # Best core best_core = np.argsort(get_proba(X, lmd, order=5))[::-1][:n_sample] train_X, train_y = X[best_core], y_poly[best_core] tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;b&#39;, label=&#39;best coreset (???)&#39;) # Active learning samples train_X, train_y = X[train_idx], y_poly[train_idx] tmpGP = GPy.models.GPRegression(train_X, train_y, kernel) tmpGP.likelihood.variance = sigma_n**2 _, var = tmpGP.predict(X_new) std2 = np.sqrt(var)*2 _ = plt.plot(X_new, std2, color=&#39;y&#39;, label=&#39;Uncert. Sampling&#39;) plt.ylim(0,5); plt.xlabel(&quot;X&quot;);plt.ylabel(&quot;Uncertainty&quot;); plot_essentials(); Interpretation of the above plot is as the following, Models trained on full dataset, a coreset and dataset generated by uncertainty sampling have more or less similar uncertainty. Model fitted on \\(n&#39;\\) points that have highest probability in coreset sampling distribution have higher uncertanty in central region. This is because we have higher density of datapoints in the central region and all points with high probability lie on both ends. This suggests that, it is not a good strategy to choose highly probable points as coresets. We must sample the points properly with a sampling technique according to the sampling distribution. We see that uncertainty sampling results in best uncertainty here that is closest to oracle (full data fit). This raises following questions for the future research. 5.4 Questions What can we say about uncertainty in the fit while fitting the model on the coreset points? We saw that uncertainty sampling can sample the points that help in reducing uncertainty the most so can we say they can produce better coresets? "],["Dsqr.html", "Coresets: 6 Appendix 1: D\\(^2\\) Sampling", " Coresets: 6 Appendix 1: D\\(^2\\) Sampling The following algorithm is known as D\\(^2\\) sampling. It was proposed by Arthur and Vassilvitskii (2007) to improve the initialization of KMeans algorithm. We also leverage the same algorithm in coreset construction of KMeans algorithm (Section 3-4). Algorithm 1: \\(D^2\\) sampling Require: dataset \\(X\\), number of clusters \\(K\\). Sample \\(x\\) from \\(X\\) uniform randomly or preavailable weights. set \\(Q_B=\\{x\\}\\) for i \\(\\to\\) \\(2, 3, ..., K\\) do sample \\(x\\) from \\(X\\) with probability \\(p(x) = \\frac{d(x, Q_B)^2}{\\sum\\limits_{x&#39; \\in X}d(x&#39;, Q_B)^2}\\) and add \\(x\\) to \\(Q_B\\). return \\(Q_B\\) Intuitively, this algorithm tries to select centers which are far away from the already selected centers. Let us try D\\(^2\\) sampling on dummy data. from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from scipy.spatial.distance import cdist, pdist import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib import rc from matplotlib.animation import FuncAnimation import warnings warnings.filterwarnings(&#39;ignore&#39;) rc(&#39;font&#39;, size=16) rc(&#39;text&#39;, usetex=True) def plot_essentials(): # essential code for every plot hand, labs = plt.gca().get_legend_handles_labels() if len(hand)&gt;0: plt.legend(hand, labs); plt.tight_layout(); plt.show() plt.close() Genereting pseudo-data, X, y = make_blobs(n_samples=200, n_features=2, centers=8, random_state=1, cluster_std=0.5) plt.scatter(X[:,0], X[:,1], c=y); plt.xlabel(&#39;$x_1$&#39;);plt.ylabel(&#39;$x_2$&#39;); plt.title(&#39;Pseudo-data with 8 clusters&#39;); plot_essentials(); Now we visualize the D\\(^2\\) algorithm, np.random.seed(0) K = 8 N = len(X) first_idx = np.random.choice(N) # Choosing first center randomly B = [] # Approximate cluster centers B.append(X[first_idx]) for choice in range(K-1): # Choice of remaining K-1 centers proba = np.square(cdist(X, np.array(B))).min(axis=1) norm_proba = proba/np.sum(proba) idx = np.random.choice(N, p=norm_proba) B.append(X[idx,:]) B = np.array(B) # Plotting fig, ax = plt.subplots(); def update(i): ax.cla(); ax.scatter(X[:,0], X[:,1], c=y, label=&#39;Data points&#39;); ax.scatter(B[:i,0], B[:i,1], c=&#39;r&#39;, marker=&#39;*&#39;, s=200, label=&#39;Selected centers&#39;); ax.legend(); fig.tight_layout(); plt.close(); anim = FuncAnimation(fig, update, frames=range(8)); rc(&#39;animation&#39;, html=&#39;html5&#39;) anim.save(&#39;Dsqr.gif&#39;, fps=2) "],["future-directions.html", "Coresets: 7 Future Directions", " Coresets: 7 Future Directions Several future ideas to worn upon are the following, Can we decompose a kernel \\(K\\) into feature matrix \\(X\\) to leverage into linear regression coreset algorithm? This way we would be creating a coreset for any Gaussian process models in general. Practical comparison (time-complexty, performance, number of coreset points) of coresets created by classical algorithms and active learning strategies. "],["acknowledgement.html", "Coresets: 8 Acknowledgement", " Coresets: 8 Acknowledgement I thank Prof. Anirban Dasgupta for constantly guiding me to explore novel directions of research in coresets during this semester. I also thank my colleague and friend Jayesh Malaviya for being a constant companion in the discussions throughout this project. I would like to acknowledge instant help whenever I required from my senior PhD colleagues Rachit Chhaya and Supratim Shit. "],["references.html", "Coresets: 9 References", " Coresets: 9 References Abras, Guillermo N, and Virginia Laura Balları́n. 2005. “A Weighted k-Means Algorithm Applied to Brain Tissue Classification.” Journal of Computer Science &amp; Technology 5. Arthur, David, and Sergei Vassilvitskii. 2007. “K-Means++: The Advantages of Careful Seeding.” In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, 1027–35. SODA ’07. USA: Society for Industrial; Applied Mathematics. Bachem, Olivier, Mario Lucic, and Andreas Krause. 2017. “Practical Coreset Constructions for Machine Learning.” http://arxiv.org/abs/1703.06476. "]]
