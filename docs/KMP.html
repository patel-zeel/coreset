<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Coresets: 4 Coresets for KMeans (Practical) | IN792: Coresets</title>
  <meta name="description" content="Coresets: 4 Coresets for KMeans (Practical) | IN792: Coresets" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Coresets: 4 Coresets for KMeans (Practical) | IN792: Coresets" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Coresets: 4 Coresets for KMeans (Practical) | IN792: Coresets" />
  
  
  

<meta name="author" content="Zeel B Patel" />


<meta name="date" content="2021-05-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="KM.html"/>
<link rel="next" href="coresets-for-linear-regression.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Coresets</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="coresets.html"><a href="coresets.html"><i class="fa fa-check"></i><b>2</b> Coresets</a>
<ul>
<li class="chapter" data-level="2.1" data-path="coresets.html"><a href="coresets.html#what-are-coresets"><i class="fa fa-check"></i><b>2.1</b> What are coresets?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="KM.html"><a href="KM.html"><i class="fa fa-check"></i><b>3</b> Coresets for KMeans (Theory)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="KM.html"><a href="KM.html#problem-formulation"><i class="fa fa-check"></i><b>3.1</b> Problem formulation</a></li>
<li class="chapter" data-level="3.2" data-path="KM.html"><a href="KM.html#uniform-sampling"><i class="fa fa-check"></i><b>3.2</b> Uniform sampling</a></li>
<li class="chapter" data-level="3.3" data-path="KM.html"><a href="KM.html#importance-sampling"><i class="fa fa-check"></i><b>3.3</b> Importance sampling</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="KM.html"><a href="KM.html#sensitivity"><i class="fa fa-check"></i><b>3.3.1</b> Sensitivity</a></li>
<li class="chapter" data-level="3.3.2" data-path="KM.html"><a href="KM.html#rough-approximation"><i class="fa fa-check"></i><b>3.3.2</b> Rough approximation</a></li>
<li class="chapter" data-level="3.3.3" data-path="KM.html"><a href="KM.html#bounding-sensitivity"><i class="fa fa-check"></i><b>3.3.3</b> Bounding sensitivity</a></li>
<li class="chapter" data-level="3.3.4" data-path="KM.html"><a href="KM.html#algorithm-to-create-kmeans-coreset"><i class="fa fa-check"></i><b>3.3.4</b> Algorithm to create KMeans coreset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="KMP.html"><a href="KMP.html"><i class="fa fa-check"></i><b>4</b> Coresets for KMeans (Practical)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="KMP.html"><a href="KMP.html#pseudo-data-with-4-clusters"><i class="fa fa-check"></i><b>4.1</b> Pseudo-data with 4 clusters</a></li>
<li class="chapter" data-level="4.2" data-path="KMP.html"><a href="KMP.html#pseudo-dataset-with-100-clusters-and-10000-data-points"><i class="fa fa-check"></i><b>4.2</b> Pseudo-dataset with 100 clusters and 10000 data-points</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="coresets-for-linear-regression.html"><a href="coresets-for-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Coresets for Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="coresets-for-linear-regression.html"><a href="coresets-for-linear-regression.html#coresets-for-linear-regression-order1"><i class="fa fa-check"></i><b>5.1</b> Coresets for linear regression (order=1)</a></li>
<li class="chapter" data-level="5.2" data-path="coresets-for-linear-regression.html"><a href="coresets-for-linear-regression.html#coresets-for-linear-regression-order5"><i class="fa fa-check"></i><b>5.2</b> Coresets for linear regression (order=5)</a></li>
<li class="chapter" data-level="5.3" data-path="coresets-for-linear-regression.html"><a href="coresets-for-linear-regression.html#checking-uncertainty-in-the-fit"><i class="fa fa-check"></i><b>5.3</b> Checking uncertainty in the fit</a></li>
<li class="chapter" data-level="5.4" data-path="coresets-for-linear-regression.html"><a href="coresets-for-linear-regression.html#questions"><i class="fa fa-check"></i><b>5.4</b> Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Dsqr.html"><a href="Dsqr.html"><i class="fa fa-check"></i><b>6</b> Appendix 1: D<span class="math inline">\(^2\)</span> Sampling</a></li>
<li class="chapter" data-level="7" data-path="future-directions.html"><a href="future-directions.html"><i class="fa fa-check"></i><b>7</b> Future Directions</a></li>
<li class="chapter" data-level="8" data-path="acknowledgement.html"><a href="acknowledgement.html"><i class="fa fa-check"></i><b>8</b> Acknowledgement</a></li>
<li class="chapter" data-level="9" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>9</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">IN792: Coresets</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="KMP" class="section level1" number="4">
<h1><span class="header-section-number">Coresets: 4</span> Coresets for KMeans (Practical)</h1>
<p>In this practical, we will try to create an <span class="math inline">\(\varepsilon\)</span>-coreset (<span class="math inline">\(\varepsilon=0.3\)</span>) for KMeans clustering.</p>
<p>Let us import some packages and set up a few useful functions,</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="KMP.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb2-2"><a href="KMP.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb2-3"><a href="KMP.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist, pdist</span>
<span id="cb2-4"><a href="KMP.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-5"><a href="KMP.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="KMP.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-7"><a href="KMP.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> rc</span>
<span id="cb2-8"><a href="KMP.html#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb2-9"><a href="KMP.html#cb2-9" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">&#39;ignore&#39;</span>)</span>
<span id="cb2-10"><a href="KMP.html#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="KMP.html#cb2-11" aria-hidden="true" tabindex="-1"></a>rc(<span class="st">&#39;font&#39;</span>, size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-12"><a href="KMP.html#cb2-12" aria-hidden="true" tabindex="-1"></a>rc(<span class="st">&#39;text&#39;</span>, usetex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-13"><a href="KMP.html#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_essentials(): <span class="co"># essential code for every plot</span></span>
<span id="cb2-14"><a href="KMP.html#cb2-14" aria-hidden="true" tabindex="-1"></a>  hand, labs <span class="op">=</span> plt.gca().get_legend_handles_labels()</span>
<span id="cb2-15"><a href="KMP.html#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">len</span>(hand)<span class="op">&gt;</span><span class="dv">0</span>:</span>
<span id="cb2-16"><a href="KMP.html#cb2-16" aria-hidden="true" tabindex="-1"></a>    plt.legend(hand, labs)<span class="op">;</span></span>
<span id="cb2-17"><a href="KMP.html#cb2-17" aria-hidden="true" tabindex="-1"></a>  plt.tight_layout()<span class="op">;</span></span>
<span id="cb2-18"><a href="KMP.html#cb2-18" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb2-19"><a href="KMP.html#cb2-19" aria-hidden="true" tabindex="-1"></a>  plt.close()</span></code></pre></div>
<div id="pseudo-data-with-4-clusters" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Pseudo-data with 4 clusters</h2>
<p>Now, we generate a pseudo-dataset with 4 clusters.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="KMP.html#cb3-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-2"><a href="KMP.html#cb3-2" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb3-3"><a href="KMP.html#cb3-3" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-4"><a href="KMP.html#cb3-4" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb3-5"><a href="KMP.html#cb3-5" aria-hidden="true" tabindex="-1"></a>c4X, c4y <span class="op">=</span> make_blobs(n_samples<span class="op">=</span>N, centers<span class="op">=</span>K, n_features<span class="op">=</span>d, random_state<span class="op">=</span><span class="dv">0</span>, cluster_std<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb3-6"><a href="KMP.html#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="KMP.html#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(c4X[:,<span class="dv">0</span>], c4X[:,<span class="dv">1</span>], c<span class="op">=</span>c4y)<span class="op">;</span></span>
<span id="cb3-8"><a href="KMP.html#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;$x_1$&#39;</span>)<span class="op">;</span>plt.ylabel(<span class="st">&#39;$x_2$&#39;</span>)<span class="op">;</span></span>
<span id="cb3-9"><a href="KMP.html#cb3-9" aria-hidden="true" tabindex="-1"></a>plot_essentials()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>We run the KMeans algorithm on the full dataset to calculate <span class="math inline">\(cost(X, Q^*_X)\)</span>. Note that, in practice, this is infeasible because of superlinear time-complexity, but we perform this step to have a comparison between <span class="math inline">\(cost(X, Q^*_X)\)</span> and <span class="math inline">\(cost(X, Q^*_C)\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="KMP.html#cb4-1" aria-hidden="true" tabindex="-1"></a>full_model <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>K, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-2"><a href="KMP.html#cb4-2" aria-hidden="true" tabindex="-1"></a>full_model.fit(c4X)<span class="op">;</span></span>
<span id="cb4-3"><a href="KMP.html#cb4-3" aria-hidden="true" tabindex="-1"></a>cost_QX <span class="op">=</span> full_model.inertia_<span class="op">/</span>N</span></code></pre></div>
<p>Optimal cost on the full dataset is <span class="math inline">\(cost(X, Q^*_X)=\)</span> 1.16.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="KMP.html#cb5-1" aria-hidden="true" tabindex="-1"></a>Q_X <span class="op">=</span> full_model.cluster_centers_</span>
<span id="cb5-2"><a href="KMP.html#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="KMP.html#cb5-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(c4X[:,<span class="dv">0</span>], c4X[:,<span class="dv">1</span>], c<span class="op">=</span>c4y)<span class="op">;</span></span>
<span id="cb5-4"><a href="KMP.html#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(Q_X[:,<span class="dv">0</span>], Q_X[:,<span class="dv">1</span>], s<span class="op">=</span><span class="dv">200</span>, marker<span class="op">=</span><span class="st">&#39;*&#39;</span>, label<span class="op">=</span><span class="st">&#39;$Q^*_X$&#39;</span>, c<span class="op">=</span><span class="st">&#39;green&#39;</span>)<span class="op">;</span></span>
<span id="cb5-5"><a href="KMP.html#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;$x_1$&#39;</span>)<span class="op">;</span>plt.ylabel(<span class="st">&#39;$x_2$&#39;</span>)<span class="op">;</span></span>
<span id="cb5-6"><a href="KMP.html#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Optimal cluster centers on the original dataset&#39;</span>)<span class="op">;</span></span>
<span id="cb5-7"><a href="KMP.html#cb5-7" aria-hidden="true" tabindex="-1"></a>plot_essentials()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The next step is running the <span class="math inline">\(D^2\)</span> sampling algorithm on the full dataset to get the approximate centers <span class="math inline">\(Q^*_B\)</span>. To ensure that <span class="math inline">\((\alpha, \beta)\)</span> criterion holds with probability <span class="math inline">\(0.9\)</span>, we need to run <span class="math inline">\(D^2\)</span> sampling <span class="math inline">\(\log_2\left(\frac{1}{1-0.9}\right) \approx 4\)</span> times and select the best clustering (least cost).</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="KMP.html#cb6-1" aria-hidden="true" tabindex="-1"></a>cost_QB <span class="op">=</span> np.inf</span>
<span id="cb6-2"><a href="KMP.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> trial <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb6-3"><a href="KMP.html#cb6-3" aria-hidden="true" tabindex="-1"></a>  np.random.seed(trial)</span>
<span id="cb6-4"><a href="KMP.html#cb6-4" aria-hidden="true" tabindex="-1"></a>  fst_idx <span class="op">=</span> np.random.choice(N) <span class="co"># Choosing first center randomly</span></span>
<span id="cb6-5"><a href="KMP.html#cb6-5" aria-hidden="true" tabindex="-1"></a>  B <span class="op">=</span> [] <span class="co"># Approximate cluster centers</span></span>
<span id="cb6-6"><a href="KMP.html#cb6-6" aria-hidden="true" tabindex="-1"></a>  B.append(c4X[fst_idx])</span>
<span id="cb6-7"><a href="KMP.html#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> choice <span class="kw">in</span> <span class="bu">range</span>(K<span class="op">-</span><span class="dv">1</span>): <span class="co"># Choice of remaining K-1 centers</span></span>
<span id="cb6-8"><a href="KMP.html#cb6-8" aria-hidden="true" tabindex="-1"></a>    proba <span class="op">=</span> np.square(cdist(c4X, np.array(B))).<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-9"><a href="KMP.html#cb6-9" aria-hidden="true" tabindex="-1"></a>    norm_proba <span class="op">=</span> proba<span class="op">/</span>np.<span class="bu">sum</span>(proba)</span>
<span id="cb6-10"><a href="KMP.html#cb6-10" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.random.choice(N, p<span class="op">=</span>norm_proba)</span>
<span id="cb6-11"><a href="KMP.html#cb6-11" aria-hidden="true" tabindex="-1"></a>    B.append(c4X[idx,:])</span>
<span id="cb6-12"><a href="KMP.html#cb6-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-13"><a href="KMP.html#cb6-13" aria-hidden="true" tabindex="-1"></a>  tmp_cost <span class="op">=</span> np.square(cdist(c4X, np.array(B)).<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">1</span>)).<span class="bu">sum</span>()<span class="op">/</span>N</span>
<span id="cb6-14"><a href="KMP.html#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> tmp_cost<span class="op">&lt;</span>cost_QB:</span>
<span id="cb6-15"><a href="KMP.html#cb6-15" aria-hidden="true" tabindex="-1"></a>    cost_QB <span class="op">=</span> tmp_cost</span>
<span id="cb6-16"><a href="KMP.html#cb6-16" aria-hidden="true" tabindex="-1"></a>    B_star <span class="op">=</span> B.copy()</span></code></pre></div>
<p>As per the <span class="math inline">\((\alpha, \beta)\)</span> criterion, <span class="math inline">\(cost(X,Q^*_B) \le \alpha\cdot cost(X,Q^*_X)\)</span>, where <span class="math inline">\(\alpha=16(\log_2K+2)cost(X,Q^*_X)\)</span>. Thus, ratio <span class="math inline">\(R(Q^*_B, Q^*_X) = \frac{cost(X,Q^*_B)}{cost(X,Q^*_X)} \le \alpha\)</span>. Lower the ratio, better the approximation we have. Let us see how much ratio we get experimentally.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="KMP.html#cb7-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">16</span><span class="op">*</span>(np.log2(K) <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb7-2"><a href="KMP.html#cb7-2" aria-hidden="true" tabindex="-1"></a>alpha_R <span class="op">=</span> cost_QB<span class="op">/</span>cost_QX<span class="op">;</span></span></code></pre></div>
<p><span class="math inline">\(\alpha=\)</span> 64 and experimental ratio is 1.59. The experimental ratio is much lower than the upper limit, which is expected given the well-separated clusters and small data. Let us visualize the approximate centers <span class="math inline">\(Q^*_B\)</span>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="KMP.html#cb8-1" aria-hidden="true" tabindex="-1"></a>Q_B <span class="op">=</span> np.array(B_star)</span>
<span id="cb8-2"><a href="KMP.html#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="KMP.html#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(c4X[:,<span class="dv">0</span>], c4X[:,<span class="dv">1</span>], c<span class="op">=</span>c4y, alpha<span class="op">=</span><span class="fl">0.5</span>)<span class="op">;</span></span>
<span id="cb8-4"><a href="KMP.html#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(Q_X[:,<span class="dv">0</span>], Q_X[:,<span class="dv">1</span>], s<span class="op">=</span><span class="dv">200</span>, marker<span class="op">=</span><span class="st">&#39;*&#39;</span>, c<span class="op">=</span><span class="st">&#39;green&#39;</span>, label<span class="op">=</span><span class="st">&#39;$Q^*_X$&#39;</span>)<span class="op">;</span></span>
<span id="cb8-5"><a href="KMP.html#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(Q_B[:,<span class="dv">0</span>], Q_B[:,<span class="dv">1</span>], s<span class="op">=</span><span class="dv">200</span>, marker<span class="op">=</span><span class="st">&#39;d&#39;</span>, c<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;$Q^*_B$&#39;</span>)<span class="op">;</span></span>
<span id="cb8-6"><a href="KMP.html#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;$x_1$&#39;</span>)<span class="op">;</span>plt.ylabel(<span class="st">&#39;$x_2$&#39;</span>)<span class="op">;</span></span>
<span id="cb8-7"><a href="KMP.html#cb8-7" aria-hidden="true" tabindex="-1"></a>plot_essentials()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Now, we will perform the importance sampling to select the coreset points. The following are the essential variables that we need to compute to calculate sensitivity and probability for the importance sampling.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="KMP.html#cb9-1" aria-hidden="true" tabindex="-1"></a>B_y <span class="op">=</span> cdist(c4X, Q_B).argmin(axis<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span> <span class="co"># Cluster labels according to Q_B</span></span>
<span id="cb9-2"><a href="KMP.html#cb9-2" aria-hidden="true" tabindex="-1"></a>cost_QB <span class="op">=</span> np.square(cdist(c4X, Q_B)).<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span> <span class="co"># Cost(X, Q_B)</span></span>
<span id="cb9-3"><a href="KMP.html#cb9-3" aria-hidden="true" tabindex="-1"></a>mean_cost_QB <span class="op">=</span> np.mean(cost_QB)<span class="op">;</span></span>
<span id="cb9-4"><a href="KMP.html#cb9-4" aria-hidden="true" tabindex="-1"></a>cost_QB_cluster <span class="op">=</span> [cost_QB[B_y<span class="op">==</span>C].mean() <span class="cf">for</span> C <span class="kw">in</span> <span class="bu">range</span>(K)]<span class="op">;</span> <span class="co"># Cost of each cluster</span></span>
<span id="cb9-5"><a href="KMP.html#cb9-5" aria-hidden="true" tabindex="-1"></a>cluster_n <span class="op">=</span> pd.Series(B_y).sort_index().value_counts().values <span class="co"># Cardinality of each cluster</span></span>
<span id="cb9-6"><a href="KMP.html#cb9-6" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="dv">6</span><span class="op">*</span>alpha <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>K<span class="op">;</span> <span class="co"># Total sensitivity</span></span></code></pre></div>
<p>We can calculate the sensitivity and probability for each data-point as following,</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="KMP.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sensitivity</span></span>
<span id="cb10-2"><a href="KMP.html#cb10-2" aria-hidden="true" tabindex="-1"></a>s_x <span class="op">=</span> np.array([<span class="dv">2</span><span class="op">*</span>alpha<span class="op">*</span>cost_QB[i]<span class="op">/</span>mean_cost_QB  <span class="op">+\</span></span>
<span id="cb10-3"><a href="KMP.html#cb10-3" aria-hidden="true" tabindex="-1"></a>                <span class="dv">4</span><span class="op">*</span>alpha<span class="op">*</span>cost_QB_cluster[B_y[i]]<span class="op">/</span>mean_cost_QB <span class="op">+\</span></span>
<span id="cb10-4"><a href="KMP.html#cb10-4" aria-hidden="true" tabindex="-1"></a>                <span class="dv">4</span><span class="op">*</span>N<span class="op">/</span>cluster_n[B_y[i]] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N)])</span>
<span id="cb10-5"><a href="KMP.html#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability</span></span>
<span id="cb10-6"><a href="KMP.html#cb10-6" aria-hidden="true" tabindex="-1"></a>q_x <span class="op">=</span> s_x<span class="op">/</span>S<span class="op">/</span>N</span>
<span id="cb10-7"><a href="KMP.html#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">sum</span>(q_x).<span class="bu">round</span>(<span class="dv">2</span>) <span class="op">==</span> <span class="fl">1.</span></span>
<span id="cb10-8"><a href="KMP.html#cb10-8" aria-hidden="true" tabindex="-1"></a>q_x <span class="op">=</span> q_x<span class="op">/</span>q_x.<span class="bu">sum</span>() <span class="co"># Adjusting for numerical precision to make sum(q)=1</span></span></code></pre></div>
<p>Let us visualize the probabilities of the data-points.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="KMP.html#cb11-1" aria-hidden="true" tabindex="-1"></a>map_ax <span class="op">=</span> plt.scatter(c4X[:,<span class="dv">0</span>], c4X[:,<span class="dv">1</span>], c<span class="op">=</span>q_x)<span class="op">;</span></span>
<span id="cb11-2"><a href="KMP.html#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.colorbar(map_ax)<span class="op">;</span></span>
<span id="cb11-3"><a href="KMP.html#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;$x_1$&#39;</span>)<span class="op">;</span>plt.ylabel(<span class="st">&#39;$x_2$&#39;</span>)<span class="op">;</span></span>
<span id="cb11-4"><a href="KMP.html#cb11-4" aria-hidden="true" tabindex="-1"></a>plot_essentials()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>We have completed all the pre-requisite steps for importance sampling. Now, let us perform the importance sampling. Initially, we will set the coreset size to only 10% of the total data-points.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="KMP.html#cb12-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb12-2"><a href="KMP.html#cb12-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span><span class="op">*</span>N) <span class="co"># Number of points to draw (coreset size)</span></span>
<span id="cb12-3"><a href="KMP.html#cb12-3" aria-hidden="true" tabindex="-1"></a>C_idx <span class="op">=</span> np.random.choice(N, size<span class="op">=</span>C<span class="op">+</span>K, p<span class="op">=</span>q_x) <span class="co"># Coreset index</span></span>
<span id="cb12-4"><a href="KMP.html#cb12-4" aria-hidden="true" tabindex="-1"></a>c4C <span class="op">=</span> c4X[C_idx] <span class="co"># Coreset points</span></span>
<span id="cb12-5"><a href="KMP.html#cb12-5" aria-hidden="true" tabindex="-1"></a>Cw <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>q_x[C_idx] <span class="co"># Corresponding weights </span></span></code></pre></div>
<p>Now, we can run the Weighted KMeans algorithm <span class="citation"><a href="references.html#ref-abras2005weighted" role="doc-biblioref">Abras and Balları́n</a> (<a href="references.html#ref-abras2005weighted" role="doc-biblioref">2005</a>)</span> to get unbiased cluster centers <span class="math inline">\(Q^*_C\)</span>.
After that, we can visually see the minimum number of samples for which the coreset property holds (when <span class="math inline">\(Q^*_C\)</span> becomes a good approximation for <span class="math inline">\(Q^*_X\)</span> according to the definition of coresets).</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="KMP.html#cb13-1" aria-hidden="true" tabindex="-1"></a>cost_QC <span class="op">=</span> [] <span class="co"># cost(X, Q_C)</span></span>
<span id="cb13-2"><a href="KMP.html#cb13-2" aria-hidden="true" tabindex="-1"></a>cost_QC_biased <span class="op">=</span> []</span>
<span id="cb13-3"><a href="KMP.html#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_points <span class="kw">in</span> <span class="bu">range</span>(K, C<span class="op">+</span>K):</span>
<span id="cb13-4"><a href="KMP.html#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Unbiased estimate</span></span>
<span id="cb13-5"><a href="KMP.html#cb13-5" aria-hidden="true" tabindex="-1"></a>  tmp_model <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>K, random_state<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb13-6"><a href="KMP.html#cb13-6" aria-hidden="true" tabindex="-1"></a>  tmp_model <span class="op">=</span> tmp_model.fit(c4C[:n_points], sample_weight<span class="op">=</span>Cw[:n_points])<span class="op">;</span></span>
<span id="cb13-7"><a href="KMP.html#cb13-7" aria-hidden="true" tabindex="-1"></a>  Q_C <span class="op">=</span> tmp_model.cluster_centers_</span>
<span id="cb13-8"><a href="KMP.html#cb13-8" aria-hidden="true" tabindex="-1"></a>  tmp_cost <span class="op">=</span> np.square(cdist(c4X, Q_C)).<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">1</span>).<span class="bu">sum</span>()<span class="op">/</span>N</span>
<span id="cb13-9"><a href="KMP.html#cb13-9" aria-hidden="true" tabindex="-1"></a>  cost_QC.append(tmp_cost)</span>
<span id="cb13-10"><a href="KMP.html#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Biased estimate</span></span>
<span id="cb13-11"><a href="KMP.html#cb13-11" aria-hidden="true" tabindex="-1"></a>  tmp_model_biased <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>K, random_state<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb13-12"><a href="KMP.html#cb13-12" aria-hidden="true" tabindex="-1"></a>  tmp_model_biased <span class="op">=</span> tmp_model_biased.fit(c4C[:n_points])<span class="op">;</span></span>
<span id="cb13-13"><a href="KMP.html#cb13-13" aria-hidden="true" tabindex="-1"></a>  Q_C_biased <span class="op">=</span> tmp_model_biased.cluster_centers_</span>
<span id="cb13-14"><a href="KMP.html#cb13-14" aria-hidden="true" tabindex="-1"></a>  tmp_cost_biased <span class="op">=</span> np.square(cdist(c4X, Q_C_biased)).<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">1</span>).<span class="bu">sum</span>()<span class="op">/</span>N</span>
<span id="cb13-15"><a href="KMP.html#cb13-15" aria-hidden="true" tabindex="-1"></a>  cost_QC_biased.append(tmp_cost_biased)</span>
<span id="cb13-16"><a href="KMP.html#cb13-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-17"><a href="KMP.html#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(K, C<span class="op">+</span>K), np.array(cost_QC)<span class="op">/</span>cost_QX, <span class="st">&#39;o-&#39;</span>, markersize<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">&#39;unbiased ratio&#39;</span>)<span class="op">;</span></span>
<span id="cb13-18"><a href="KMP.html#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(K, C<span class="op">+</span>K), np.array(cost_QC_biased)<span class="op">/</span>cost_QX, <span class="st">&#39;o-&#39;</span>, markersize<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">&#39;biased ratio&#39;</span>)<span class="op">;</span></span>
<span id="cb13-19"><a href="KMP.html#cb13-19" aria-hidden="true" tabindex="-1"></a>plt.hlines(<span class="dv">1</span>, <span class="op">*</span>plt.xlim(), label<span class="op">=</span><span class="st">&#39;$1$&#39;</span>)<span class="op">;</span></span>
<span id="cb13-20"><a href="KMP.html#cb13-20" aria-hidden="true" tabindex="-1"></a>plt.hlines(<span class="dv">1</span><span class="op">+</span><span class="dv">3</span><span class="op">*</span>epsilon, <span class="op">*</span>plt.xlim(), label<span class="op">=</span><span class="st">&#39;$1+3 </span><span class="ch">\\</span><span class="st">varepsilon $&#39;</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>)<span class="op">;</span></span>
<span id="cb13-21"><a href="KMP.html#cb13-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Number of coreset points&#39;</span>)<span class="op">;</span>plt.ylabel(<span class="st">&#39;$</span><span class="ch">\\</span><span class="st">frac{cost(X, Q_C)}{cost(X, Q_X)}$&#39;</span>)<span class="op">;</span></span>
<span id="cb13-22"><a href="KMP.html#cb13-22" aria-hidden="true" tabindex="-1"></a>plot_essentials()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>We see that the coreset property holds at a substantially smaller number of points. However, the number of coreset points does not depend on the number of data-points <span class="math inline">\(N\)</span>, but it depends on the number of clusters <span class="math inline">\(K\)</span>, dimension of data <span class="math inline">\(d\)</span>, <span class="math inline">\(\varepsilon\)</span> and minimum probability <span class="math inline">\(\delta\)</span> for coreset. The upper bound defined by <span class="citation"><a href="references.html#ref-bachem2017practical" role="doc-biblioref">Bachem, Lucic, and Krause</a> (<a href="references.html#ref-bachem2017practical" role="doc-biblioref">2017</a>)</span> suggests drawing nearly 2000 points for our current settings, but we can see that, in practice, coreset property holds with a much lesser number of coreset points. Theoretical bound is given as the following,</p>
<p><span class="math display">\[
\text{Number of coreset points } m = \Omega\left( \frac{dK^3\log(K)+K^2\log(\frac{1}{\delta})}{\varepsilon^2}\right) 
\]</span></p>
<p>We will draw a coreset with a 20% size of the original dataset and fix it as the coreset for further analysis.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="KMP.html#cb14-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb14-2"><a href="KMP.html#cb14-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span><span class="op">*</span>N) <span class="co"># Number of points to draw (coreset size)</span></span>
<span id="cb14-3"><a href="KMP.html#cb14-3" aria-hidden="true" tabindex="-1"></a>C_idx <span class="op">=</span> np.random.choice(N, size<span class="op">=</span>C<span class="op">+</span>K, p<span class="op">=</span>q_x) <span class="co"># Coreset index</span></span>
<span id="cb14-4"><a href="KMP.html#cb14-4" aria-hidden="true" tabindex="-1"></a>c4C <span class="op">=</span> c4X[C_idx] <span class="co"># Coreset points</span></span>
<span id="cb14-5"><a href="KMP.html#cb14-5" aria-hidden="true" tabindex="-1"></a>Cw <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>q_x[C_idx] <span class="co"># Corresponding weights </span></span></code></pre></div>
<p>Now, let us visualize the cluster centers <span class="math inline">\(Q^*_C\)</span> found by running KMeans on the coreset and cluster centers <span class="math inline">\(Q^*_X\)</span> found by running KMeans on the original dataset (again, just to demonstrate).</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="KMP.html#cb15-1" aria-hidden="true" tabindex="-1"></a>full_model <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>K, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-2"><a href="KMP.html#cb15-2" aria-hidden="true" tabindex="-1"></a>full_model <span class="op">=</span> full_model.fit(c4X)</span>
<span id="cb15-3"><a href="KMP.html#cb15-3" aria-hidden="true" tabindex="-1"></a>cost_QX <span class="op">=</span> <span class="op">-</span>full_model.score(c4X) <span class="co"># cost(X, Q_X) (score is opposite of cost) </span></span>
<span id="cb15-4"><a href="KMP.html#cb15-4" aria-hidden="true" tabindex="-1"></a>Q_X <span class="op">=</span> full_model.cluster_centers_</span>
<span id="cb15-5"><a href="KMP.html#cb15-5" aria-hidden="true" tabindex="-1"></a>X_labels <span class="op">=</span> full_model.predict(c4X)</span>
<span id="cb15-6"><a href="KMP.html#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="KMP.html#cb15-7" aria-hidden="true" tabindex="-1"></a>coreset_model <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>K, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-8"><a href="KMP.html#cb15-8" aria-hidden="true" tabindex="-1"></a>coreset_model <span class="op">=</span> coreset_model.fit(c4C, sample_weight<span class="op">=</span>Cw)</span>
<span id="cb15-9"><a href="KMP.html#cb15-9" aria-hidden="true" tabindex="-1"></a>cost_QC <span class="op">=</span> <span class="op">-</span>coreset_model.score(c4X) <span class="co"># cost(X, Q_C)</span></span>
<span id="cb15-10"><a href="KMP.html#cb15-10" aria-hidden="true" tabindex="-1"></a>Q_C <span class="op">=</span> coreset_model.cluster_centers_</span>
<span id="cb15-11"><a href="KMP.html#cb15-11" aria-hidden="true" tabindex="-1"></a>C_labels <span class="op">=</span> coreset_model.predict(c4X)</span>
<span id="cb15-12"><a href="KMP.html#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="KMP.html#cb15-13" aria-hidden="true" tabindex="-1"></a>colors1 <span class="op">=</span> [<span class="st">&#39;tab:blue&#39;</span>, <span class="st">&#39;tab:orange&#39;</span>, <span class="st">&#39;tab:brown&#39;</span>, <span class="st">&#39;m&#39;</span>]</span>
<span id="cb15-14"><a href="KMP.html#cb15-14" aria-hidden="true" tabindex="-1"></a>mapper1 <span class="op">=</span> <span class="kw">lambda</span> x: [colors1[i] <span class="cf">for</span> i <span class="kw">in</span> x]</span>
<span id="cb15-15"><a href="KMP.html#cb15-15" aria-hidden="true" tabindex="-1"></a>colors2 <span class="op">=</span> [<span class="st">&#39;tab:orange&#39;</span>, <span class="st">&#39;tab:brown&#39;</span>, <span class="st">&#39;tab:blue&#39;</span>, <span class="st">&#39;m&#39;</span>]</span>
<span id="cb15-16"><a href="KMP.html#cb15-16" aria-hidden="true" tabindex="-1"></a>mapper2 <span class="op">=</span> <span class="kw">lambda</span> x: [colors2[i] <span class="cf">for</span> i <span class="kw">in</span> x]</span>
<span id="cb15-17"><a href="KMP.html#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="KMP.html#cb15-18" aria-hidden="true" tabindex="-1"></a>plt.scatter(c4X[:,<span class="dv">0</span>], c4X[:,<span class="dv">1</span>], c<span class="op">=</span>mapper1(X_labels), marker<span class="op">=</span><span class="st">&#39;&gt;&#39;</span>, label<span class="op">=</span><span class="st">&#39;clusters of $Q^*_X$&#39;</span>, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb15-19"><a href="KMP.html#cb15-19" aria-hidden="true" tabindex="-1"></a>plt.scatter(c4X[:,<span class="dv">0</span>], c4X[:,<span class="dv">1</span>], c<span class="op">=</span>mapper2(C_labels), marker<span class="op">=</span><span class="st">&#39;&lt;&#39;</span>, label<span class="op">=</span><span class="st">&#39;clusters of $Q^*_C$&#39;</span>, s<span class="op">=</span><span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb15-20"><a href="KMP.html#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="KMP.html#cb15-21" aria-hidden="true" tabindex="-1"></a>plt.scatter(Q_X[:,<span class="dv">0</span>], Q_X[:,<span class="dv">1</span>], c<span class="op">=</span><span class="st">&#39;k&#39;</span>, marker<span class="op">=</span><span class="st">&#39;&gt;&#39;</span>, s<span class="op">=</span><span class="dv">100</span>, label<span class="op">=</span><span class="st">&#39;$Q^*_X$&#39;</span>)<span class="op">;</span></span>
<span id="cb15-22"><a href="KMP.html#cb15-22" aria-hidden="true" tabindex="-1"></a>plt.scatter(Q_C[:,<span class="dv">0</span>], Q_C[:,<span class="dv">1</span>], c<span class="op">=</span><span class="st">&#39;k&#39;</span>, marker<span class="op">=</span><span class="st">&#39;&lt;&#39;</span>, s<span class="op">=</span><span class="dv">100</span>, label<span class="op">=</span><span class="st">&#39;$Q^*_C$&#39;</span>)<span class="op">;</span></span>
<span id="cb15-23"><a href="KMP.html#cb15-23" aria-hidden="true" tabindex="-1"></a>plot_essentials()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p><span class="math inline">\(cost(X, Q^*_X)=\)</span> 116<br />
<span class="math inline">\(cost(X, Q^*_C)=\)</span> 156<br />
<span class="math inline">\(R=\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\)</span> 1.343 <span class="math inline">\(&lt; \frac{1+\varepsilon}{1-\varepsilon}=\)</span> 1.857</p>
<p>We can see that <span class="math inline">\(Q^*_C\)</span> can approximate the <span class="math inline">\(Q^*_X\)</span> efficiently as per our requirement of <span class="math inline">\(\varepsilon\)</span>-coreset.</p>
</div>
<div id="pseudo-dataset-with-100-clusters-and-10000-data-points" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Pseudo-dataset with 100 clusters and 10000 data-points</h2>
<p>Let us try the coreset construction on a more extensive dataset. First, we will generate a dataset with 100 clusters and 10000 points. I have hidden the code for better visuals and to avoid redundancy.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Now, let us check the actual fit with vanilla KMeans algorithm,</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Now, we find approximate centers <span class="math inline">\(Q^*_B\)</span> with <span class="math inline">\(D^2\)</span> sampling,</p>
<p><span class="math inline">\(\alpha=\)</span> 138.3 and experimental ratio is 1.8.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Let us visualize the probability distribution over all data points.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Now, we visualize comparison between <span class="math inline">\(cost(C,Q)\)</span> and <span class="math inline">\(cost(X,Q)\)</span> as we increase <span class="math inline">\(m\)</span>.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>We see that <span class="math inline">\(cost(C, Q)\)</span> starts satisfying coreset property well below 10% of the total dataset. Let us fix the coreset size at 10% of the entire dataset and check the effect on the coreset property.</p>
<p><span class="math inline">\(cost(X, Q^*_X)=\)</span> 1585<br />
<span class="math inline">\(cost(X, Q^*_C)=\)</span> 1876<br />
<span class="math inline">\(R=\frac{cost(X, Q^*_C)}{cost(X, Q^*_X)}=\)</span> 1.183 <span class="math inline">\(&lt; \frac{1+\varepsilon}{1-\varepsilon}=\)</span> 1.857</p>
<p>We can see that <span class="math inline">\(Q^*_C\)</span> can approximate the <span class="math inline">\(Q^*_X\)</span> efficiently as per our requirement of <span class="math inline">\(\varepsilon\)</span>-coreset.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="KM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="coresets-for-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/patel-zeel/coreset/edit/master/Chap-3-KMeans-practical.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/patel-zeel/coreset/blob/master/Chap-3-KMeans-practical.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
